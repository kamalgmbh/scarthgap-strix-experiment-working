From 675dcac3a36fb60d84dbe2440015f19729c056e8 Mon Sep 17 00:00:00 2001
From: Dmitry Osipenko <dmitry.osipenko@collabora.com>
Date: Sun, 29 Jan 2023 03:42:37 +0300
Subject: [PATCH 26/70] wip: virtio-gpu: Support async+per-ctx fences

Signed-off-by: Dmitry Osipenko <dmitry.osipenko@collabora.com>
---
 hw/display/virtio-gpu-virgl.c  | 242 ++++++++++++++++++++++++++++++++-
 include/hw/virtio/virtio-gpu.h |   5 +
 2 files changed, 246 insertions(+), 1 deletion(-)

diff --git a/hw/display/virtio-gpu-virgl.c b/hw/display/virtio-gpu-virgl.c
index 411644d667..7796d61106 100644
--- a/hw/display/virtio-gpu-virgl.c
+++ b/hw/display/virtio-gpu-virgl.c
@@ -78,6 +78,9 @@ virgl_gpu_find_resource(VirtIOGPU *g, uint32_t resource_id)
     return container_of(res, struct virgl_gpu_resource, res);
 }
 
+static bool use_async_cb = true;
+static bool use_per_ctx_fence = true;
+
 #if VIRGL_RENDERER_CALLBACKS_VERSION >= 4
 static void *
 virgl_get_egl_display(G_GNUC_UNUSED void *cookie)
@@ -886,7 +889,141 @@ void virtio_gpu_virgl_process_cmd(VirtIOGPU *g,
     }
 
     trace_virtio_gpu_fence_ctrl(cmd->cmd_hdr.fence_id, cmd->cmd_hdr.type);
-    virgl_renderer_create_fence(cmd->cmd_hdr.fence_id, cmd->cmd_hdr.type);
+
+    if (use_per_ctx_fence && (cmd->cmd_hdr.flags & VIRTIO_GPU_FLAG_INFO_RING_IDX)) {
+        uint32_t flags = 0;
+
+        virgl_renderer_context_create_fence(cmd->cmd_hdr.ctx_id, flags,
+                                            cmd->cmd_hdr.ring_idx,
+                                            cmd->cmd_hdr.fence_id);
+    } else {
+        virgl_renderer_create_fence(cmd->cmd_hdr.fence_id, 0);
+    }
+}
+
+static int virtio_gpu_virgl_fence_read(VirtIOGPU *g, uint64_t *value)
+{
+    ssize_t ret;
+
+    do {
+        ret = read(g->read_pipe, value, sizeof(*value));
+    } while ((ret == -1 && errno == EINTR));
+
+    if (ret < 0) {
+        if (errno != EAGAIN)
+            error_report("%s: failed: %s", __func__, strerror(errno));
+        return -errno;
+    }
+
+    return 0;
+}
+
+static int virtio_gpu_virgl_fence_write(VirtIOGPU *g, uint64_t value)
+{
+    ssize_t ret;
+
+    do {
+        ret = write(g->write_pipe, &value, sizeof(value));
+    } while (ret < 0 && (errno == EINTR || errno == EAGAIN));
+
+    if (ret < 0) {
+        error_report("%s: failed: %s", __func__, strerror(errno));
+        return ret;
+    }
+
+    return 0;
+}
+
+static void virtio_gpu_virgl_fence_event(void *opaque)
+{
+    struct virtio_gpu_ctrl_command *cmd;
+    VirtIOGPU *g = opaque;
+    uint64_t fence;
+
+    QTAILQ_FOREACH(cmd, &g->fenceq, next) {
+        if (!virtio_queue_ready(cmd->vq)) {
+            return;
+        }
+    }
+
+    while (!virtio_gpu_virgl_fence_read(g, &fence)) {
+        QTAILQ_FOREACH(cmd, &g->fenceq, next) {
+            /*
+             * the guest can end up emitting fences out of order
+             * so we should check all fenced cmds not just the first one.
+             */
+            if (cmd->cmd_hdr.fence_id > fence) {
+                continue;
+            }
+            trace_virtio_gpu_fence_resp(cmd->cmd_hdr.fence_id);
+            virtio_gpu_ctrl_response_nodata(g, cmd, VIRTIO_GPU_RESP_OK_NODATA);
+            QTAILQ_REMOVE(&g->fenceq, cmd, next);
+            g_free(cmd);
+            g->inflight--;
+            if (virtio_gpu_stats_enabled(g->parent_obj.conf)) {
+                fprintf(stderr, "inflight: %3d (-)\r", g->inflight);
+            }
+        }
+    }
+}
+
+struct context_fence {
+    uint32_t ctx_id;
+    uint64_t queue_id;
+    uint64_t fence_id;
+    uint32_t ctx_fence;
+};
+
+static int
+virtio_gpu_virgl_write_context_fence(VirtIOGPU *g, uint32_t ctx_id,
+                                     uint32_t queue_id, uint64_t fence_id,
+                                     bool ctx_fence)
+{
+    struct context_fence *f = g_malloc(sizeof(*f));
+
+    f->ctx_id = ctx_id;
+    f->queue_id = queue_id;
+    f->fence_id = fence_id;
+    f->ctx_fence = ctx_fence;
+
+    int err = virtio_gpu_virgl_fence_write(g, (uintptr_t)f);
+    if (err) {
+        g_free(f);
+        return err;
+    }
+
+    return 0;
+}
+
+static int
+virtio_gpu_virgl_read_context_fence(VirtIOGPU *g, uint32_t *ctx_id,
+                                     uint64_t *queue_id, uint64_t *fence_id,
+                                     uint32_t *ctx_fence)
+{
+    struct context_fence *f;
+    uint64_t ptr;
+
+    int err = virtio_gpu_virgl_fence_read(g, &ptr);
+    if (err)
+        return err;
+
+    f = (void *)ptr;
+    *ctx_id = f->ctx_id;
+    *queue_id = f->queue_id;
+    *fence_id = f->fence_id;
+    *ctx_fence = f->ctx_fence;
+
+    g_free(f);
+
+    return 0;
+}
+
+static void virgl_write_fence_async(VirtIOGPU *g, uint32_t fence)
+{
+    if (use_per_ctx_fence)
+        virtio_gpu_virgl_write_context_fence(g, 0, 0, fence, false);
+    else
+        virtio_gpu_virgl_fence_write(g, fence);
 }
 
 static void virgl_write_fence(void *opaque, uint32_t fence)
@@ -894,6 +1031,9 @@ static void virgl_write_fence(void *opaque, uint32_t fence)
     VirtIOGPU *g = opaque;
     struct virtio_gpu_ctrl_command *cmd, *tmp;
 
+    if (use_async_cb)
+        return virgl_write_fence_async(g, fence);
+
     QTAILQ_FOREACH_SAFE(cmd, &g->fenceq, next, tmp) {
         /*
          * the guest can end up emitting fences out of order
@@ -913,6 +1053,62 @@ static void virgl_write_fence(void *opaque, uint32_t fence)
     }
 }
 
+static void virtio_gpu_virgl_context_fence_event(void *opaque)
+{
+    struct virtio_gpu_ctrl_command *cmd;
+    VirtIOGPU *g = opaque;
+    uint64_t fence_id;
+    uint64_t queue_id;
+    uint32_t ctx_id;
+    uint32_t ctx_fence;
+
+    QTAILQ_FOREACH(cmd, &g->fenceq, next) {
+        if (!virtio_queue_ready(cmd->vq)) {
+            return;
+        }
+    }
+
+    while (!virtio_gpu_virgl_read_context_fence(g, &ctx_id, &queue_id,
+                                                &fence_id, &ctx_fence)) {
+        QTAILQ_FOREACH(cmd, &g->fenceq, next) {
+            /*
+             * the guest can end up emitting fences out of order
+             * so we should check all fenced cmds not just the first one.
+             */
+            if (cmd->cmd_hdr.fence_id > fence_id) {
+                continue;
+            }
+            if (!!(cmd->cmd_hdr.flags & VIRTIO_GPU_FLAG_INFO_RING_IDX) ^ !!ctx_fence) {
+                continue;
+            }
+            if (cmd->cmd_hdr.flags & VIRTIO_GPU_FLAG_INFO_RING_IDX) {
+                if (cmd->cmd_hdr.ring_idx != queue_id) {
+                    continue;
+                }
+                if (cmd->cmd_hdr.ctx_id != ctx_id) {
+                    continue;
+                }
+            }
+            trace_virtio_gpu_fence_resp(cmd->cmd_hdr.fence_id);
+            virtio_gpu_ctrl_response_nodata(g, cmd, VIRTIO_GPU_RESP_OK_NODATA);
+            QTAILQ_REMOVE(&g->fenceq, cmd, next);
+            g_free(cmd);
+            g->inflight--;
+            if (virtio_gpu_stats_enabled(g->parent_obj.conf)) {
+                fprintf(stderr, "inflight: %3d (-)\r", g->inflight);
+            }
+        }
+    }
+}
+
+static void virgl_write_context_fence(void *opaque, uint32_t ctx_id,
+                                      uint32_t queue_id, uint64_t fence_id)
+{
+    VirtIOGPU *g = opaque;
+
+    virtio_gpu_virgl_write_context_fence(g, ctx_id, queue_id, fence_id, true);
+}
+
 static virgl_renderer_gl_context
 virgl_create_context(void *opaque, int scanout_idx,
                      struct virgl_renderer_gl_ctx_param *params)
@@ -1005,6 +1201,33 @@ void virtio_gpu_virgl_reset(VirtIOGPU *g)
     virgl_renderer_reset();
 }
 
+static int virtio_gpu_virgl_init_pipe(VirtIOGPU *g)
+{
+    int fds[2];
+    int ret;
+
+    if (!g_unix_open_pipe(fds, FD_CLOEXEC, NULL)) {
+        return -errno;
+    }
+    if (!g_unix_set_fd_nonblocking(fds[0], true, NULL)) {
+        ret = -errno;
+        goto fail;
+    }
+    if (!g_unix_set_fd_nonblocking(fds[1], true, NULL)) {
+        ret = -errno;
+        goto fail;
+    }
+    g->read_pipe = fds[0];
+    g->write_pipe = fds[1];
+
+    return 0;
+
+fail:
+    close(fds[0]);
+    close(fds[1]);
+    return ret;
+}
+
 int virtio_gpu_virgl_init(VirtIOGPU *g)
 {
     int ret;
@@ -1015,6 +1238,12 @@ int virtio_gpu_virgl_init(VirtIOGPU *g)
         virtio_gpu_3d_cbs.version = 4;
         virtio_gpu_3d_cbs.get_egl_display = virgl_get_egl_display;
     }
+    virtio_gpu_3d_cbs.write_context_fence = virgl_write_context_fence;
+
+    if (use_async_cb)
+        flags |= VIRGL_RENDERER_ASYNC_FENCE_CB;
+    if (use_per_ctx_fence)
+        flags |= VIRGL_RENDERER_THREAD_SYNC;
 #endif
 
 #ifdef VIRGL_RENDERER_VENUS
@@ -1027,6 +1256,17 @@ int virtio_gpu_virgl_init(VirtIOGPU *g)
         return ret;
     }
 
+    ret = virtio_gpu_virgl_init_pipe(g);
+    if (ret != 0) {
+        error_report("fence notifier could not be initialized: %d", ret);
+        return ret;
+    }
+
+    if (use_per_ctx_fence)
+        qemu_set_fd_handler(g->read_pipe, virtio_gpu_virgl_context_fence_event, NULL, g);
+    else
+        qemu_set_fd_handler(g->read_pipe, virtio_gpu_virgl_fence_event, NULL, g);
+
     g->fence_poll = timer_new_ms(QEMU_CLOCK_VIRTUAL,
                                  virtio_gpu_fence_poll, g);
 
diff --git a/include/hw/virtio/virtio-gpu.h b/include/hw/virtio/virtio-gpu.h
index a175f80116..37fc2521c8 100644
--- a/include/hw/virtio/virtio-gpu.h
+++ b/include/hw/virtio/virtio-gpu.h
@@ -207,6 +207,11 @@ struct VirtIOGPU {
     GHashTable *resource_uuids;
 
     uint32_t supported_capset_ids[4];
+
+    int read_pipe;
+    int write_pipe;
+
+    QemuMutex pipe_lock;
 };
 
 struct VirtIOGPUClass {
-- 
2.17.1

