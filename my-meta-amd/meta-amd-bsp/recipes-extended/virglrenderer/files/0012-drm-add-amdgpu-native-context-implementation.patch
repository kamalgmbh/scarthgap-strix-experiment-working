From 9bc0b3798a280229f6098ef9a3fd1e7dc38b54ee Mon Sep 17 00:00:00 2001
From: Pierre-Eric Pelloux-Prayer <pierre-eric.pelloux-prayer@amd.com>
Date: Fri, 6 Oct 2023 11:31:46 +0200
Subject: [PATCH 12/16] drm: add amdgpu native-context implementation

Reviewed-by: Rob Clark <robdclark@gmail.com>
---
 config.h.meson                       |    1 +
 meson.build                          |   16 +-
 meson_options.txt                    |    7 +
 src/drm/amdgpu/amdgpu_renderer.c     | 1435 ++++++++++++++++++++++++++
 src/drm/amdgpu/amdgpu_renderer.h     |   25 +
 src/drm/amdgpu/amdgpu_virtio_proto.h |  205 ++++
 src/drm/drm-uapi/amdgpu_drm.h        | 1200 +++++++++++++++++++++
 src/drm/drm-uapi/drm.h               | 1216 ++++++++++++++++++++++
 src/drm/drm_renderer.c               |   12 +
 src/drm/drm_renderer.h               |    2 +-
 src/drm_hw.h                         |   16 +-
 src/meson.build                      |   11 +
 src/virglrenderer.c                  |    2 +-
 src/virglrenderer.h                  |    2 +-
 src/vrend_decode.c                   |    2 +-
 15 files changed, 4146 insertions(+), 6 deletions(-)
 create mode 100644 src/drm/amdgpu/amdgpu_renderer.c
 create mode 100644 src/drm/amdgpu/amdgpu_renderer.h
 create mode 100644 src/drm/amdgpu/amdgpu_virtio_proto.h
 create mode 100644 src/drm/drm-uapi/amdgpu_drm.h
 create mode 100644 src/drm/drm-uapi/drm.h

diff --git a/config.h.meson b/config.h.meson
index 479dfbfa..a61e9d4e 100644
--- a/config.h.meson
+++ b/config.h.meson
@@ -38,6 +38,7 @@
 #mesondefine ENABLE_GBM
 #mesondefine ENABLE_DRM
 #mesondefine ENABLE_DRM_MSM
+#mesondefine ENABLE_DRM_AMDGPU
 #mesondefine ENABLE_RENDER_SERVER
 #mesondefine ENABLE_RENDER_SERVER_WORKER_PROCESS
 #mesondefine ENABLE_RENDER_SERVER_WORKER_THREAD
diff --git a/meson.build b/meson.build
index 9f91ff2f..df670814 100644
--- a/meson.build
+++ b/meson.build
@@ -78,6 +78,12 @@ prog_python = import('python').find_installation('python3')
 
 not_found = dependency('', required: false)
 libdrm_dep = dependency('libdrm', version : '>=2.4.50', required: get_option('drm').enabled() or get_option('venus'))
+libdrm_amdgpu_dep = dependency('libdrm_amdgpu', version : '>=2.4.50')
+if not cc.has_function('amdgpu_device_initialize2',
+                        dependencies : libdrm_amdgpu_dep,
+                        prefix : '#include <amdgpu.h>')
+   error('amdgpu_device_initialize2 is missing from libdrm_amdgpu (make sure to use the right branch)')
+endif
 gbm_dep = not_found
 thread_dep = dependency('threads')
 epoxy_dep = dependency('epoxy', version: '>= 1.5.4')
@@ -272,7 +278,14 @@ with_drm_msm = have_vla and get_option('drm-msm-experimental')
 if with_drm_msm
   conf_data.set('ENABLE_DRM_MSM', 1)
 endif
-with_drm = with_drm_msm
+
+with_drm_amdgpu = have_vla and get_option('drm-amdgpu-experimental')
+if with_drm_amdgpu
+  conf_data.set('ENABLE_DRM', 1)
+  conf_data.set('ENABLE_DRM_AMDGPU', 1)
+endif
+
+with_drm = with_drm_msm or with_drm_amdgpu
 
 with_check_gl_errors = get_option('check-gl-errors')
 if with_check_gl_errors
@@ -372,6 +385,7 @@ summary({'c_args': (' ').join(get_option('c_args')),
         'venus': with_venus,
         'drm-msm': with_drm_msm,
         'render server (DEPRECATED)': with_render_server,
+        'drm-amdgpu': with_drm_amdgpu,
         'render server worker': with_render_server ? with_render_server_worker : 'none',
         'video': with_video,
         'tests': with_tests,
diff --git a/meson_options.txt b/meson_options.txt
index 84f6a977..838d34be 100644
--- a/meson_options.txt
+++ b/meson_options.txt
@@ -73,6 +73,13 @@ option(
   description : 'enable support for msm drm native context'
 )
 
+option(
+  'drm-amdgpu-experimental',
+  type : 'boolean',
+  value : 'false',
+  description : 'enable support for amdgpu drm native context'
+)
+
 option(
   'render-server',
   type : 'boolean',
diff --git a/src/drm/amdgpu/amdgpu_renderer.c b/src/drm/amdgpu/amdgpu_renderer.c
new file mode 100644
index 00000000..44e579d4
--- /dev/null
+++ b/src/drm/amdgpu/amdgpu_renderer.c
@@ -0,0 +1,1435 @@
+/*
+ * Copyright 2023 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "amdgpu_renderer.h"
+
+#include "util/anon_file.h"
+#include "util/bitscan.h"
+#include "util/hash_table.h"
+#include "util/macros.h"
+#include "util/u_atomic.h"
+#include "util/u_math.h"
+#include "pipe/p_state.h"
+#include "amdgpu_virtio_proto.h"
+#include "u_thread.h"
+#include "virgl_context.h"
+#include "virglrenderer.h"
+#include "vrend_renderer.h"
+
+#include <errno.h>
+#include <fcntl.h>
+#include <string.h>
+#include <unistd.h>
+#include <sys/mman.h>
+
+#include <xf86drm.h>
+
+#include <amdgpu.h>
+#include "drm_util.h"
+#include "drm_fence.h"
+
+#if defined(__clang__)
+#pragma GCC diagnostic ignored "-Wgnu-zero-variadic-macro-arguments"
+#endif
+
+#define print(level, fmt, ...) do {       \
+   if (ctx->debug >= level) { \
+      unsigned c = (unsigned)((uintptr_t)ctx >> 8) % 256; \
+      printf("\033[0;38;5;%dm", c);     \
+      printf("[%d|%s] %s: " fmt "\n", ctx->base.ctx_id, ctx->debug_name, __FUNCTION__, ##__VA_ARGS__); \
+      printf("\033[0m");     \
+   } \
+ } while (false)
+
+struct cmd_stat {
+   uint64_t total_duration_us;
+   uint32_t min_duration_us, max_duration_us;
+   uint32_t count;
+};
+
+struct amdgpu_context {
+   struct virgl_context base;
+
+   const char *debug_name;
+
+   struct amdvgpu_shmem *shmem;
+   uint64_t shmem_size;
+   uint32_t shm_res_id;
+   uint8_t *rsp_mem;
+   uint32_t rsp_mem_sz;
+
+   struct amdgpu_ccmd_rsp *current_rsp;
+
+   int fd;
+
+   struct hash_table *blob_table;
+   struct hash_table *resource_table;
+
+   int eventfd;
+
+   amdgpu_device_handle dev;
+   int debug;
+
+   struct hash_table_u64 *id_to_ctx;
+
+   struct {
+      struct cmd_stat s[8];
+      uint64_t last_print_ms;
+   } statistics;
+
+   uint32_t timeline_count;
+   struct drm_timeline timelines[];
+};
+DEFINE_CAST(virgl_context, amdgpu_context)
+
+static
+int close_fd(struct amdgpu_context *ctx, int fd, const char *from) {
+   print(2, "close_fd %d (%s)", fd, from);
+   return close(fd);
+}
+
+int
+amdgpu_renderer_probe(int fd, struct virgl_renderer_capset_drm *capset)
+{
+   amdgpu_device_handle dev;
+   uint32_t drm_major, drm_minor;
+   int r;
+   r = amdgpu_device_initialize2(fd, false, &drm_major, &drm_minor, &dev);
+   if (r)
+      return -ENOTSUP;
+   amdgpu_query_sw_info(dev, amdgpu_sw_info_address32_hi,
+                        &capset->u.amdgpu.address32_hi);
+   amdgpu_query_buffer_size_alignment(dev,
+                                      &capset->u.amdgpu.alignments);
+   amdgpu_query_gpu_info(dev,
+                         &capset->u.amdgpu.gpu_info);
+   strncpy(capset->u.amdgpu.marketing_name,
+           amdgpu_get_marketing_name(dev),
+           sizeof(capset->u.amdgpu.marketing_name) - 1);
+
+   amdgpu_device_deinitialize(dev);
+
+   return 0;
+}
+
+/* Imported objects will use this blob id. */
+#define UNKOWN_BLOB_ID 0xffffffff
+
+struct amdgpu_object {
+   /* amdgpu_drm handle to the object. */
+   amdgpu_bo_handle bo;
+   /* Global, assigned by guest kernel. */
+   uint32_t res_id;
+   /* Context-specific, assigned by guest userspace. It's used to link the bo
+    * created via AMDGPU_CCMD_GEM_NEW and the get_blob() callback.
+    */
+   uint32_t blob_id;
+   /* KMS handle, used in ioctl (eg: amdgpu_cs_submit_raw2). */
+   uint32_t kms_handle;
+
+   uint32_t flags;
+   uint32_t size;
+   bool exported   : 1;
+   bool cpu_mapped : 1;
+   bool detached   : 1; /* true if detach_resource was called */
+
+   uint64_t va;
+
+   unsigned vm_flags;
+};
+
+static void free_amdgpu_object(struct amdgpu_context *ctx, struct amdgpu_object *obj);
+
+static void
+amdgpu_renderer_destroy_fini(struct amdgpu_context *ctx)
+{
+   print(2, "real destroy");
+
+   for (unsigned i = 0; i < ctx->timeline_count; i++) {
+      drm_timeline_fini(&ctx->timelines[i]);
+      free((char*)ctx->timelines[i].name);
+   }
+
+   close_fd(ctx, ctx->eventfd, "amdgpu_renderer_destroy eventfd");
+
+   if (ctx->shmem) {
+      print(2, "Unmap shmem %p", (void*)ctx->shmem);
+      munmap(ctx->shmem, ctx->shmem_size);
+      ctx->shmem = NULL;
+   }
+
+   amdgpu_device_deinitialize(ctx->dev);
+
+   free((void*)ctx->debug_name);
+   memset(ctx, 0, sizeof(struct amdgpu_context));
+   free(ctx);
+}
+
+static void
+amdgpu_renderer_destroy(struct virgl_context *vctx)
+{
+   struct amdgpu_context *ctx = to_amdgpu_context(vctx);
+
+   /* Safety check */
+   if (_mesa_hash_table_num_entries(ctx->resource_table) != 0) {
+      print(2, "%d resources leaked:",
+         _mesa_hash_table_num_entries(ctx->resource_table));
+      hash_table_foreach (ctx->resource_table, entry) {
+         struct amdgpu_object *o = entry->data;
+         print(2, "  * blob_id: %u res_id: %u mapped: %p",
+            o->blob_id, o->res_id, (void*)o->cpu_mapped);
+      }
+   }
+   amdgpu_renderer_destroy_fini(ctx);
+}
+
+static void *
+amdgpu_context_rsp_noshadow(struct amdgpu_context *ctx, const struct vdrm_ccmd_req *hdr)
+{
+   return &ctx->rsp_mem[hdr->rsp_off];
+}
+
+static void *
+amdgpu_context_rsp(struct amdgpu_context *ctx, const struct vdrm_ccmd_req *hdr,
+                   unsigned len)
+{
+   unsigned rsp_mem_sz = ctx->rsp_mem_sz;
+   unsigned off = hdr->rsp_off;
+
+   if ((off > rsp_mem_sz) || (len > rsp_mem_sz - off)) {
+      print(0, "invalid shm offset: off=%u, len=%u (shmem_size=%u)", off, len, rsp_mem_sz);
+      return NULL;
+   }
+
+   struct amdgpu_ccmd_rsp *rsp = amdgpu_context_rsp_noshadow(ctx, hdr);
+
+   assert(len >= sizeof(*rsp));
+
+   /* With newer host and older guest, we could end up wanting a larger rsp struct
+    * than guest expects, so allocate a shadow buffer in this case rather than
+    * having to deal with this in all the different ccmd handlers.  This is similar
+    * in a way to what drm_ioctl() does.
+    */
+   if (len > rsp->base.len) {
+      rsp = malloc(len);
+      if (!rsp)
+         return NULL;
+      rsp->base.len = len;
+   }
+
+   ctx->current_rsp = rsp;
+
+   return rsp;
+}
+
+static struct amdgpu_object *
+amdgpu_object_create(amdgpu_bo_handle handle, uint32_t flags, uint32_t size)
+{
+   struct amdgpu_object *obj = calloc(1, sizeof(*obj));
+
+   if (!obj)
+      return NULL;
+
+   obj->blob_id = UNKOWN_BLOB_ID;
+   obj->bo = handle;
+   obj->flags = flags;
+   obj->size = size;
+
+   return obj;
+}
+
+static struct hash_entry *
+table_search(struct hash_table *ht, uint32_t key)
+{
+   /* zero is not a valid key for u32_keys hashtable: */
+   if (!key)
+      return NULL;
+   return _mesa_hash_table_search(ht, (void *)(uintptr_t)key);
+}
+
+static bool
+valid_blob_id(struct amdgpu_context *ctx, uint32_t blob_id)
+{
+   /* must be non-zero: */
+   if (blob_id == 0)
+      return false;
+
+   /* must not already be in-use: */
+   if (table_search(ctx->blob_table, blob_id))
+      return false;
+
+   return true;
+}
+
+static void
+amdgpu_object_set_blob_id(struct amdgpu_context *ctx, struct amdgpu_object *obj,
+                          uint32_t blob_id)
+{
+   obj->blob_id = blob_id;
+   _mesa_hash_table_insert(ctx->blob_table, (void *)(uintptr_t)obj->blob_id, obj);
+}
+
+static struct amdgpu_object *
+amdgpu_retrieve_object_from_blob_id(struct amdgpu_context *ctx, uint64_t blob_id)
+{
+   assert((blob_id >> 32) == 0);
+   uint32_t id = blob_id;
+   struct hash_entry *entry = table_search(ctx->blob_table, id);
+   if (!entry)
+      return NULL;
+   struct amdgpu_object *obj = entry->data;
+   _mesa_hash_table_remove(ctx->blob_table, entry);
+   return obj;
+}
+
+static struct amdgpu_object *
+amdgpu_get_object_from_res_id(struct amdgpu_context *ctx, uint32_t res_id, const char *from)
+{
+   const struct hash_entry *entry = table_search(ctx->resource_table, res_id);
+   if (likely(entry)) {
+      return entry->data;
+   } else {
+      if (from) {
+         print(0, "Couldn't find res_id: %u [%s]", res_id, from);
+         hash_table_foreach (ctx->resource_table, entry) {
+            struct amdgpu_object *o = entry->data;
+            print(1, "  * blob_id: %u res_id: %u", o->blob_id, o->res_id);
+         }
+      }
+      return NULL;
+   }
+}
+
+static bool
+valid_res_id(struct amdgpu_context *ctx, uint32_t res_id)
+{
+   return !table_search(ctx->resource_table, res_id);
+}
+
+static void
+amdgpu_object_set_res_id(struct amdgpu_context *ctx, struct amdgpu_object *obj,
+                         uint32_t res_id)
+{
+   assert(valid_res_id(ctx, res_id));
+
+   obj->res_id = res_id;
+
+   print(2, "blob_id=%u, res_id: %u, va=%" PRIx64, obj->blob_id, obj->res_id,
+          obj->va);
+
+   _mesa_hash_table_insert(ctx->resource_table, (void *)(uintptr_t)obj->res_id, obj);
+}
+
+static void
+amdgpu_remove_object(struct amdgpu_context *ctx, struct amdgpu_object *obj)
+{
+   _mesa_hash_table_remove_key(ctx->resource_table, (void *)(uintptr_t)obj->res_id);
+}
+
+static void
+amdgpu_renderer_attach_resource(struct virgl_context *vctx, struct virgl_resource *res)
+{
+   struct amdgpu_context *ctx = to_amdgpu_context(vctx);
+   struct amdgpu_object *obj = amdgpu_get_object_from_res_id(ctx, res->res_id, NULL);
+
+   if (!obj) {
+      if (res->fd_type == VIRGL_RESOURCE_FD_SHM) {
+         assert(res->res_id == ctx->shm_res_id);
+         return;
+      }
+
+      int fd;
+      enum virgl_resource_fd_type fd_type = virgl_resource_export_fd(res, &fd);
+      if (fd_type == VIRGL_RESOURCE_FD_DMABUF) {
+         struct amdgpu_bo_info info = {0};
+         struct amdgpu_bo_import_result import;
+         int ret;
+
+         ret = amdgpu_bo_import(ctx->dev, amdgpu_bo_handle_type_dma_buf_fd, fd,
+                                &import);
+
+         close_fd(ctx, fd, __FUNCTION__);
+         if (ret) {
+            print(0, "Could not import fd=%d: %s", fd, strerror(errno));
+            return;
+         }
+
+         ret = amdgpu_bo_query_info(import.buf_handle, &info);
+         if (ret) {
+            print(0, "amdgpu_bo_query_info failed\n");
+            return;
+         }
+
+         obj = amdgpu_object_create(import.buf_handle, 0, import.alloc_size);
+         if (!obj)
+            return;
+
+         obj->bo = import.buf_handle;
+         amdgpu_bo_export(obj->bo, amdgpu_bo_handle_type_kms, &obj->kms_handle);
+         amdgpu_object_set_res_id(ctx, obj, res->res_id);
+         print(1, "imported dmabuf -> res_id=%u | va: %" PRIx64, obj->res_id, obj->va);
+      } else {
+         print(2, "Ignored res_id: %d (fd_type = %d)", res->res_id, fd_type);
+         if (fd_type != VIRGL_RESOURCE_FD_INVALID)
+            close_fd(ctx, fd, __FUNCTION__);
+         return;
+      }
+   }
+}
+
+static void free_amdgpu_object(struct amdgpu_context *ctx, struct amdgpu_object *obj)
+{
+   assert(!obj->cpu_mapped);
+   print(2, "free obj res_id: %d", obj->res_id);
+   amdgpu_remove_object(ctx, obj);
+
+   /* Release the blob */
+   if (obj->va)
+      amdgpu_bo_va_op(obj->bo, 0, obj->size, obj->va, 0, AMDGPU_VA_OP_UNMAP);
+
+   amdgpu_bo_free(obj->bo);
+
+   free(obj);
+}
+
+static void
+amdgpu_renderer_detach_resource(struct virgl_context *vctx, struct virgl_resource *res)
+{
+   struct amdgpu_context *ctx = to_amdgpu_context(vctx);
+   struct amdgpu_object *obj = amdgpu_get_object_from_res_id(ctx, res->res_id, NULL);
+
+   if (!obj) {
+      /* If this context doesn't know about this resource id there's nothing to do. */
+       return;
+   }
+
+   print(2, "res_id=%u (fd_type: %d)| va: %" PRIx64, obj->res_id, res->fd_type, obj->va);
+
+   free_amdgpu_object(ctx, obj);
+}
+
+static enum virgl_resource_fd_type
+amdgpu_renderer_export_opaque_handle(struct virgl_context *vctx,
+                                     struct virgl_resource *res, int *out_fd)
+{
+   struct amdgpu_context *ctx = to_amdgpu_context(vctx);
+   struct amdgpu_object *obj = amdgpu_get_object_from_res_id(ctx, res->res_id, __FUNCTION__);
+   int ret;
+
+   print(2, "obj=%p, res_id=%u", (void *)obj, res->res_id);
+
+   if (!obj) {
+      print(0, "invalid res_id %u", res->res_id);
+      return VIRGL_RESOURCE_FD_INVALID;
+   }
+
+   ret = amdgpu_bo_export(obj->bo, amdgpu_bo_handle_type_dma_buf_fd, (uint32_t *)out_fd);
+
+   if (ret) {
+      print(0, "failed to get dmabuf fd: %s", strerror(errno));
+      return VIRGL_RESOURCE_FD_INVALID;
+   }
+
+   char dmabufname[32] = { 0 };
+   snprintf(dmabufname, sizeof(dmabufname) - 1, "e:%d-%s",
+            obj->res_id, ctx->debug_name);
+   set_dmabuf_name(*out_fd, dmabufname);
+
+   return VIRGL_RESOURCE_FD_DMABUF;
+}
+
+static int
+amdgpu_renderer_transfer_3d(UNUSED struct virgl_context *vctx,
+                            UNUSED struct virgl_resource *res,
+                            UNUSED const struct vrend_transfer_info *info,
+                            UNUSED int transfer_mode)
+{
+   struct amdgpu_context *ctx = to_amdgpu_context(vctx);
+   print(0, "unsupported");
+   return -1;
+}
+
+static void
+update_heap_info_in_shmem(struct amdgpu_context *ctx)
+{
+   amdgpu_query_heap_info(ctx->dev, AMDGPU_GEM_DOMAIN_VRAM,
+                       AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED,
+                       &ctx->shmem->vis_vram);
+   amdgpu_query_heap_info(ctx->dev, AMDGPU_GEM_DOMAIN_VRAM,
+                          0,
+                          &ctx->shmem->vram);
+   amdgpu_query_heap_info(ctx->dev, AMDGPU_GEM_DOMAIN_GTT,
+                          0,
+                          &ctx->shmem->gtt);
+}
+
+static int
+amdgpu_renderer_get_blob(struct virgl_context *vctx, uint32_t res_id, uint64_t blob_id,
+                         uint64_t blob_size, uint32_t blob_flags,
+                         struct virgl_context_blob *blob)
+{
+   struct amdgpu_context *ctx = to_amdgpu_context(vctx);
+
+   print(2, "blob_id=%" PRIu64 ", res_id=%u, blob_size=%" PRIu64
+         ", blob_flags=0x%x",
+           blob_id, res_id, blob_size, blob_flags);
+
+   if ((blob_id >> 32) != 0) {
+      print(0, "invalid blob_id: %" PRIu64, blob_id);
+      return -EINVAL;
+   }
+
+   /* blob_id of zero is reserved for the shmem buffer: */
+   if (blob_id == 0) {
+      int fd;
+
+      if (blob_flags != VIRGL_RENDERER_BLOB_FLAG_USE_MAPPABLE) {
+         print(0, "invalid blob_flags: 0x%x", blob_flags);
+         return -EINVAL;
+      }
+
+      if (ctx->shmem) {
+         print(0, "There can be only one!");
+         return -EINVAL;
+      }
+
+      char name[64];
+      snprintf(name, 64, "amdgpu-shmem-%s", ctx->debug_name);
+      fd = os_create_anonymous_file(blob_size, name);
+      if (fd < 0) {
+         print(0, "Failed to create shmem file: %s", strerror(errno));
+         return -ENOMEM;
+      }
+
+      int ret = fcntl(fd, F_ADD_SEALS, F_SEAL_SEAL | F_SEAL_SHRINK | F_SEAL_GROW);
+      if (ret) {
+         print(0, "fcntl failed: %s", strerror(errno));
+         close_fd(ctx, fd, __FUNCTION__);
+         return -ENOMEM;
+      }
+
+      ctx->shmem = mmap(NULL, blob_size, PROT_WRITE | PROT_READ, MAP_SHARED, fd, 0);
+      if (ctx->shmem == MAP_FAILED) {
+         print(0, "shmem mmap failed: %s", strerror(errno));
+         close_fd(ctx, fd, __FUNCTION__);
+         return -ENOMEM;
+      }
+      ctx->shmem_size = blob_size;
+      ctx->shm_res_id = res_id;
+      print(1, "shmem: %p (size: %lu vs %lu)", (void *)ctx->shmem, blob_size, sizeof(*ctx->shmem));
+
+      ctx->shmem->base.rsp_mem_offset = sizeof(*ctx->shmem);
+
+      uint8_t *ptr = (uint8_t *)ctx->shmem;
+      ctx->rsp_mem = &ptr[ctx->shmem->base.rsp_mem_offset];
+      ctx->rsp_mem_sz = blob_size - ctx->shmem->base.rsp_mem_offset;
+
+      blob->type = VIRGL_RESOURCE_FD_SHM;
+      blob->u.fd = fd;
+      blob->map_info = VIRGL_RENDERER_MAP_CACHE_WC;
+
+      update_heap_info_in_shmem(ctx);
+
+      return 0;
+   }
+
+   if (!valid_res_id(ctx, res_id)) {
+      print(0, "Invalid res_id %u", res_id);
+      return -EINVAL;
+   }
+
+   struct amdgpu_object *obj = amdgpu_retrieve_object_from_blob_id(ctx, blob_id);
+
+   /* If GEM_NEW fails, we can end up here without a backing obj or if it's a dumb buffer. */
+   if (!obj) {
+      print(0, "No object with blob_id=%ld", blob_id);
+      return -ENOENT;
+   }
+
+   blob->map_info = VIRGL_RENDERER_MAP_CACHE_WC;
+
+   /* a memory can only be exported once; we don't want two resources to point
+    * to the same storage.
+    */
+   if (obj->exported) {
+      print(0, "Already exported! blob_id:%ld", blob_id);
+      return -EINVAL;
+   }
+
+   amdgpu_object_set_res_id(ctx, obj, res_id);
+
+   if (blob_flags & VIRGL_RENDERER_BLOB_FLAG_USE_SHAREABLE) {
+      int fd, ret;
+
+      ret = amdgpu_bo_export(obj->bo, amdgpu_bo_handle_type_dma_buf_fd, (uint32_t *)&fd);
+
+      if (ret) {
+         print(0, "Export to fd failed for blob_id:%ld r=%d (%s)", blob_id, ret, strerror(errno));
+         return ret;
+      }
+
+      char dmabufname[32] = { 0 };
+      snprintf(dmabufname, sizeof(dmabufname) - 1, "r:%d-%s",
+               obj->res_id, ctx->debug_name);
+      set_dmabuf_name(fd, dmabufname);
+
+      print(2, "dmabuf created: %d for res_id: %d", fd, obj->res_id);
+
+      blob->type = VIRGL_RESOURCE_FD_DMABUF;
+      blob->u.fd = fd;
+   } else {
+      blob->type = VIRGL_RESOURCE_OPAQUE_HANDLE;
+      blob->u.opaque_handle = obj->kms_handle;
+   }
+
+   obj->exported = true;
+
+   /* Update usage (should probably be done on alloc/import instead). */
+   update_heap_info_in_shmem(ctx);
+
+   return 0;
+}
+
+static int
+amdgpu_ccmd_query_info(struct amdgpu_context *ctx, const struct vdrm_ccmd_req *hdr)
+{
+   const struct amdgpu_ccmd_query_info_req *req = to_amdgpu_ccmd_query_info_req(hdr);
+   struct amdgpu_ccmd_query_info_rsp *rsp;
+   unsigned rsp_len = sizeof(*rsp) + req->info.return_size;
+
+   rsp = amdgpu_context_rsp(ctx, hdr, rsp_len);
+
+   if (!rsp)
+      return -ENOMEM;
+
+   size_t return_size = MIN2(req->info.return_size, rsp_len);
+   void *value = calloc(return_size, 1);
+   struct drm_amdgpu_info request;
+   memcpy(&request, &req->info, sizeof(request));
+   request.return_pointer = (uintptr_t)value;
+
+   int r = drmCommandWrite(amdgpu_device_get_fd(ctx->dev), DRM_AMDGPU_INFO, &request, sizeof(request));
+
+   rsp->hdr.ret = r;
+
+   if (rsp->hdr.ret < 0 && request.query != AMDGPU_INFO_HW_IP_INFO)
+      print(0, "ioctl error: fd: %d r: %d|%d %s",
+            amdgpu_device_get_fd(ctx->dev), rsp->hdr.ret, r,strerror(errno));
+
+   memcpy(rsp->payload, value, req->info.return_size);
+   free(value);
+
+   return 0;
+}
+
+static int
+amdgpu_ccmd_gem_new(struct amdgpu_context *ctx, const struct vdrm_ccmd_req *hdr)
+{
+   const struct amdgpu_ccmd_gem_new_req *req = to_amdgpu_ccmd_gem_new_req(hdr);
+   int ret = 0;
+   int64_t va_map_size = 0;
+
+   if (!valid_blob_id(ctx, req->blob_id)) {
+      print(0, "Invalid blob_id %ld", req->blob_id);
+      ret = -EINVAL;
+      goto alloc_failed;
+   }
+
+   struct amdgpu_bo_alloc_request r = {
+      .alloc_size = req->r.alloc_size,
+      .phys_alignment = req->r.phys_alignment,
+      .preferred_heap = req->r.preferred_heap,
+      .flags = req->r.flags,
+   };
+   amdgpu_bo_handle bo_handle;
+   ret = amdgpu_bo_alloc(ctx->dev, &r, &bo_handle);
+
+   if (ret) {
+      print(0, "amdgpu_bo_alloc failed: %d (%s)", ret, strerror(errno));
+      goto alloc_failed;
+   }
+
+   /* If a VA address was requested, assign it to the BO now. */
+   if (req->va) {
+      va_map_size = align64(req->vm_map_size, getpagesize());
+
+      ret = amdgpu_bo_va_op_raw(ctx->dev, bo_handle, 0, va_map_size, req->va,
+                                req->vm_flags, AMDGPU_VA_OP_MAP);
+      if (ret) {
+         print(0, "amdgpu_bo_va_op_raw failed: %d va: %" PRIx64 " va_map_size: %ld (%s)",
+               ret, req->va, va_map_size, strerror(errno));
+         goto va_map_failed;
+      }
+   }
+
+   uint32_t kms_handle;
+   ret = amdgpu_bo_export(bo_handle, amdgpu_bo_handle_type_kms, &kms_handle);
+   if (ret) {
+      print(0, "Failed to get kms handle");
+      goto export_failed;
+   }
+
+   struct amdgpu_object *obj =
+      amdgpu_object_create(bo_handle, req->r.flags, req->r.alloc_size);
+   obj->kms_handle = kms_handle;
+   obj->vm_flags = req->vm_flags;
+
+   assert(obj);
+
+   obj->va = req->va;
+
+   amdgpu_object_set_blob_id(ctx, obj, req->blob_id);
+
+   print(2, "new object blob_id: %ld heap: %08x flags: %lx vm_flags: %x va: %" PRIx64,
+      req->blob_id, req->r.preferred_heap, req->r.flags, req->vm_flags, obj->va);
+
+   return 0;
+
+export_failed:
+   if (req->vm_flags)
+      amdgpu_bo_va_op(bo_handle, 0, va_map_size, req->va, 0, AMDGPU_VA_OP_UNMAP);
+
+va_map_failed:
+   amdgpu_bo_free(bo_handle);
+
+alloc_failed:
+   print(2, "ERROR blob_id: %ld heap: %08x flags: %lx vm_flags: %x va: %" PRIx64,
+         req->blob_id, req->r.preferred_heap, req->r.flags, req->vm_flags, req->va);
+   if (ctx->shmem)
+      ctx->shmem->async_error++;
+   return ret;
+}
+
+static int
+amdgpu_ccmd_bo_va_op(struct amdgpu_context *ctx, const struct vdrm_ccmd_req *hdr)
+{
+   const struct amdgpu_ccmd_bo_va_op_req *req = to_amdgpu_ccmd_bo_va_op_req(hdr);
+   struct amdgpu_object *obj;
+   struct amdgpu_ccmd_rsp *rsp;
+   rsp = amdgpu_context_rsp(ctx, hdr, sizeof(struct amdgpu_ccmd_rsp));
+
+   if (req->is_sparse_bo) {
+      rsp->ret = amdgpu_bo_va_op_raw(
+         ctx->dev, NULL, req->offset, req->vm_map_size, req->va,
+         req->flags, req->op);
+      if (rsp->ret && ctx->shmem) {
+         ctx->shmem->async_error++;
+         print(0, "amdgpu_bo_va_op_raw for sparse bo failed: offset: 0x%lx size: 0x%lx va: %" PRIx64, req->offset, req->vm_map_size, req->va);
+      }
+      return 0;
+   }
+
+   obj = amdgpu_get_object_from_res_id(ctx, req->res_id, __FUNCTION__);
+   if (!obj) {
+      /* This is ok. This means the guest closed the GEM already. */
+      return -EINVAL;
+   }
+
+   rsp->ret = amdgpu_bo_va_op_raw(
+      ctx->dev, obj->bo, req->offset, req->vm_map_size, req->va,
+      req->flags,
+      req->op);
+   if (rsp->ret) {
+      if (ctx->shmem)
+         ctx->shmem->async_error++;
+
+      print(0, "amdgpu_bo_va_op_raw failed: op: %d res_id: %d offset: 0x%lx size: 0x%lx va: %" PRIx64 " r=%d",
+         req->op, obj->res_id, req->offset, req->vm_map_size, req->va, rsp->ret);
+   } else {
+      obj->va = req->va;
+      print(2, "va_op %d res_id: %u va: %" PRIx64, req->op, req->res_id, obj->va);
+   }
+
+   return 0;
+}
+
+static int
+amdgpu_ccmd_set_metadata(struct amdgpu_context *ctx, const struct vdrm_ccmd_req *hdr)
+{
+   const struct amdgpu_ccmd_set_metadata_req *req = to_amdgpu_ccmd_set_metadata_req(hdr);
+   struct amdgpu_ccmd_rsp *rsp = amdgpu_context_rsp(ctx, hdr, sizeof(struct amdgpu_ccmd_rsp));
+
+   struct amdgpu_object *obj = amdgpu_get_object_from_res_id(ctx, req->res_id, __FUNCTION__);
+   if (!obj) {
+      print(0, "Cannot find object with res_id=%d",
+              req->res_id);
+      rsp->ret = -EINVAL;
+      return -1;
+   }
+
+   /* We could also store the metadata here instead of passing them to the host kernel =>
+    * actually no because this would only work if the desktop runs on radeonsi-virtio.
+    */
+   struct amdgpu_bo_metadata metadata = {0};
+   metadata.flags = req->flags;
+   metadata.tiling_info = req->tiling_info;
+   metadata.size_metadata = req->size_metadata;
+   if (req->size_metadata)
+      memcpy(metadata.umd_metadata, req->umd_metadata, req->size_metadata);
+
+   rsp->ret = amdgpu_bo_set_metadata(obj->bo, &metadata);
+   if (rsp->ret) {
+      print(0, "amdgpu_bo_set_metadata failed for res: %d", req->res_id);
+   }
+
+   return 0;
+}
+
+static int
+amdgpu_ccmd_bo_query_info(struct amdgpu_context *ctx, const struct vdrm_ccmd_req *hdr)
+{
+   const struct amdgpu_ccmd_bo_query_info_req *req =
+      to_amdgpu_ccmd_bo_query_info_req(hdr);
+   struct amdgpu_ccmd_bo_query_info_rsp *rsp;
+   rsp = amdgpu_context_rsp(ctx, hdr, sizeof(struct amdgpu_ccmd_bo_query_info_rsp));
+
+   struct amdgpu_object *obj = amdgpu_get_object_from_res_id(ctx, req->res_id, __FUNCTION__);
+   if (!obj) {
+      print(0, "Cannot find object");
+      rsp->hdr.ret = -EINVAL;
+      return -1;
+   }
+
+   struct amdgpu_bo_info info = {0};
+   rsp->hdr.ret = amdgpu_bo_query_info(obj->bo, &info);
+   if (rsp->hdr.ret) {
+      print(0, "amdgpu_bo_query_info failed");
+      rsp->hdr.ret = -EINVAL;
+      return -1;
+   }
+
+   memcpy(&rsp->info, &info, sizeof(info));
+
+   return 0;
+}
+
+static int
+amdgpu_ccmd_create_ctx(struct amdgpu_context *ctx, const struct vdrm_ccmd_req *hdr)
+{
+   const struct amdgpu_ccmd_create_ctx_req *req = to_amdgpu_ccmd_create_ctx_req(hdr);
+   struct amdgpu_ccmd_create_ctx_rsp *rsp;
+   rsp = amdgpu_context_rsp(ctx, hdr, sizeof(struct amdgpu_ccmd_create_ctx_rsp));
+
+   if (req->create) {
+      amdgpu_context_handle ctx_handle;
+
+      int r = amdgpu_cs_ctx_create2(ctx->dev, req->priority, &ctx_handle);
+      rsp->hdr.ret = r;
+      if (r) {
+         print(0, "amdgpu_cs_ctx_create2(prio=%d) failed (%s)", req->priority, strerror(errno));
+         return r;
+      }
+
+      print(1, "amdgpu_cs_ctx_create2 dev: %p -> %p", (void*)ctx->dev, (void*)ctx_handle);
+
+      if (!rsp)
+         return -ENOMEM;
+
+      /* We need the ctx_id in the guest */
+      struct amdgpu_cs_fence f = {
+         .context = ctx_handle
+      };
+      struct drm_amdgpu_cs_chunk_dep d = { 0 };
+      amdgpu_cs_chunk_fence_to_dep(&f, &d);
+      rsp->ctx_id = d.ctx_id;
+
+      _mesa_hash_table_u64_insert(ctx->id_to_ctx,
+                                  (uint64_t)(uintptr_t)d.ctx_id,
+                                  (void*)(uintptr_t)ctx_handle);
+
+   } else {
+      amdgpu_context_handle actx = _mesa_hash_table_u64_search(ctx->id_to_ctx,
+                                                               (uintptr_t)req->id);
+      if (actx == NULL) {
+         print(0, "Failed to find ctx_id: %d", req->id);
+      } else {
+         amdgpu_cs_ctx_free(actx);
+         rsp->hdr.ret = 0;
+        _mesa_hash_table_u64_remove(ctx->id_to_ctx, (uint64_t)(uintptr_t) req->id);
+      }
+
+      print(1, "amdgpu_cs_ctx_free dev: %p -> %p", (void*)ctx->dev, (void*)actx);
+   }
+
+   return 0;
+}
+
+static int
+amdgpu_ccmd_cs_submit(struct amdgpu_context *ctx, const struct vdrm_ccmd_req *hdr)
+{
+   const struct amdgpu_ccmd_cs_submit_req *req = to_amdgpu_ccmd_cs_submit_req(hdr);
+   struct drm_amdgpu_bo_list_in bo_list_in;
+   struct drm_amdgpu_cs_chunk_fence user_fence;
+   struct drm_amdgpu_cs_chunk chunks[req->num_chunks + 1 /* syncobj_in */ + 1 /* syncobj_out */];
+   struct drm_amdgpu_cs_chunk_sem syncobj_in = { 0 };
+   struct drm_amdgpu_bo_list_entry *bo_handles_in = NULL;
+   unsigned num_chunks = 0;
+   uint64_t seqno = 0;
+   int r;
+
+   struct amdgpu_ccmd_rsp *rsp;
+   rsp = amdgpu_context_rsp(ctx, hdr, sizeof(struct amdgpu_ccmd_rsp));
+
+   if (req->ring_idx == 0 || ctx->timeline_count < req->ring_idx) {
+      print(0, "Invalid ring_idx value: %d (must be in [1, %d] range)",
+         req->ring_idx,
+         ctx->timeline_count);
+      rsp->ret = -EINVAL;
+      return -1;
+   }
+
+   amdgpu_context_handle actx = _mesa_hash_table_u64_search(ctx->id_to_ctx,
+                                                            (uintptr_t)req->ctx_id);
+
+   struct desc {
+      uint16_t chunk_id;
+      uint16_t length_dw;
+      uint32_t offset;
+   };
+   struct desc *descriptors = (void*) req->payload;
+   struct drm_amdgpu_bo_list_entry *bo_list = NULL;
+
+   for (size_t i = 0; i < req->num_chunks; i++) {
+      unsigned chunk_id = descriptors[i].chunk_id;
+      unsigned offset = req->num_chunks * sizeof(struct desc) +
+                        descriptors[i].offset;
+
+      chunks[num_chunks].chunk_id = chunk_id;
+
+      void *input = (void*) &req->payload[offset];
+
+      if (chunk_id == AMDGPU_CHUNK_ID_BO_HANDLES) {
+         /* Don't trust blindly the inputs. */
+         size_t bo_count = MIN2(req->bo_number, req->hdr.len / 4);
+         if (bo_count != req->bo_number) {
+            r = -EINVAL;
+            goto end;
+         }
+
+         bo_handles_in = input;
+         bo_list = malloc(bo_count * sizeof(struct drm_amdgpu_bo_list_entry));
+
+         bo_list_in.operation = ~0;
+         bo_list_in.list_handle = ~0;
+         bo_list_in.bo_number = bo_count;
+         bo_list_in.bo_info_size = sizeof(struct drm_amdgpu_bo_list_entry);
+         bo_list_in.bo_info_ptr = (uint64_t)(uintptr_t)bo_list;
+
+         for (unsigned j = 0; j < req->bo_number; j++) {
+            struct amdgpu_object *obj =
+               amdgpu_get_object_from_res_id(ctx, bo_handles_in[j].bo_handle, __FUNCTION__);
+            if (!obj) {
+               print(0, "Couldn't retrieve bo with res_id %d", bo_handles_in[j].bo_handle);
+               r = -EINVAL;
+               goto end;
+            }
+            bo_list[j].bo_handle = obj->kms_handle;
+            bo_list[j].bo_priority = bo_handles_in[j].bo_priority;
+         }
+
+         chunks[num_chunks].length_dw = sizeof(struct drm_amdgpu_bo_list_in) / 4;
+         chunks[num_chunks].chunk_data = (uintptr_t)&bo_list_in;
+      } else if (chunk_id == AMDGPU_CHUNK_ID_FENCE) {
+         struct drm_amdgpu_cs_chunk_fence *in = input;
+         struct amdgpu_object *obj =
+               amdgpu_get_object_from_res_id(ctx, in->handle, __FUNCTION__);
+         if (!obj) {
+            print(0, "Couldn't retrieve user_fence bo with res_id %d", in->handle);
+            r = -EINVAL;
+            goto end;
+         }
+         struct amdgpu_cs_fence_info info;
+         info.offset = in->offset / sizeof(int64_t);
+         info.handle = obj->bo;
+         amdgpu_cs_chunk_fence_info_to_data(&info, (void*) &user_fence);
+         chunks[num_chunks].length_dw = sizeof(struct drm_amdgpu_cs_chunk_fence) / 4;
+         chunks[num_chunks].chunk_data = (uintptr_t)&user_fence;
+      } else if (chunk_id == AMDGPU_CHUNK_ID_DEPENDENCIES) {
+         chunks[num_chunks].length_dw = descriptors[i].length_dw;
+         chunks[num_chunks].chunk_data = (uintptr_t)input;
+      } else if (chunk_id == AMDGPU_CHUNK_ID_IB) {
+         chunks[num_chunks].length_dw = descriptors[i].length_dw;
+         chunks[num_chunks].chunk_data = (uintptr_t)input;
+         if (chunks[num_chunks].length_dw != sizeof(struct drm_amdgpu_cs_chunk_ib) / 4) {
+            r = -EINVAL;
+            goto end;
+         }
+      } else {
+         print(0, "Unsupported chunk_id %d received", chunk_id);
+         assert(false);
+      }
+
+      num_chunks++;
+   }
+
+   int in_fence_fd = virgl_context_take_in_fence_fd(&ctx->base);
+   if (in_fence_fd >= 0) {
+      chunks[num_chunks].chunk_id = AMDGPU_CHUNK_ID_SYNCOBJ_IN;
+      chunks[num_chunks].length_dw = sizeof(syncobj_in) / 4;
+      chunks[num_chunks].chunk_data = (uintptr_t)&syncobj_in;
+      r = drmSyncobjCreate(amdgpu_device_get_fd(ctx->dev), 0, &syncobj_in.handle);
+      if (r)
+         goto end;
+
+      r = drmSyncobjImportSyncFile(
+         amdgpu_device_get_fd(ctx->dev), syncobj_in.handle, in_fence_fd);
+      if (r == 0)
+         num_chunks++;
+      else
+         goto end;
+   }
+
+   struct drm_amdgpu_cs_chunk_sem syncobj_out = { 0 };
+   chunks[num_chunks].chunk_id = AMDGPU_CHUNK_ID_SYNCOBJ_OUT;
+   chunks[num_chunks].length_dw = sizeof(syncobj_out) / 4;
+   chunks[num_chunks].chunk_data = (uintptr_t)&syncobj_out;
+   r = drmSyncobjCreate(amdgpu_device_get_fd(ctx->dev), 0, &syncobj_out.handle);
+   if (r == 0) {
+      num_chunks++;
+   } else {
+      print(0, "out syncobj creation failed");
+      goto end;
+   }
+
+   r = amdgpu_cs_submit_raw2(ctx->dev, actx, 0, num_chunks, chunks, &seqno);
+
+   if (in_fence_fd >= 0) {
+      close(in_fence_fd);
+      drmSyncobjDestroy(amdgpu_device_get_fd(ctx->dev), syncobj_in.handle);
+   }
+
+   if (r == 0) {
+      int submit_fd;
+      r = drmSyncobjExportSyncFile(amdgpu_device_get_fd(ctx->dev), syncobj_out.handle, &submit_fd);
+      if (r == 0) {
+         drm_timeline_set_last_fence_fd(&ctx->timelines[req->ring_idx - 1], submit_fd);
+         print(3, "Set last fd ring_idx: %d: %d", req->ring_idx, submit_fd);
+      } else {
+         print(0, "Failed to create a FD from the syncobj (%d)", r);
+      }
+   } else {
+      if (ctx->shmem)
+         ctx->shmem->async_error++;
+      print(0, "command submission failed failed (ring: %d, num_chunks: %d)", req->ring_idx, num_chunks);
+   }
+
+   if (r != 0 || ctx->debug >= 4) {
+      print(1, "GPU submit used %d BOs:", req->bo_number);
+      print(1, "Used | Resource ID | VA Range");
+      print(1, "-----|-------------|---------");
+      hash_table_foreach (ctx->resource_table, entry) {
+         const struct amdgpu_object *o = entry->data;
+         bool used = false;
+         for (unsigned j = 0; j < req->bo_number && !used; j++) {
+            if (bo_handles_in[j].bo_handle == o->res_id)
+               used = true;
+         }
+
+         if (r == 0 && !used)
+            continue;
+
+         print(1, "%s | %*u | 0x%" PRIx64 " - 0x%" PRIx64,
+            used ? "  x " : "    ",
+            (int)strlen("Resource ID"), o->res_id,
+            o->va, o->va ? (o->va + o->size - 1) : 0);
+      }
+   }
+
+   drmSyncobjDestroy(amdgpu_device_get_fd(ctx->dev), syncobj_out.handle);
+
+   print(3, "ctx: %d -> seqno={v=%d a=%ld} r=%d", req->ctx_id, hdr->seqno, seqno, r);
+
+end:
+   if (bo_list)
+      free(bo_list);
+
+   rsp->ret = r;
+   return r;
+}
+
+static int amdgpu_ccmd_reserve_vmid(struct amdgpu_context *ctx, const struct vdrm_ccmd_req *hdr)
+{
+   const struct amdgpu_ccmd_reserve_vmid_req *req = to_amdgpu_ccmd_reserve_vmid_req(hdr);
+   struct amdgpu_ccmd_rsp *rsp;
+   rsp = amdgpu_context_rsp(ctx, hdr, sizeof(struct amdgpu_ccmd_rsp));
+   rsp->ret = req->enable ?
+         amdgpu_vm_reserve_vmid(ctx->dev, 0) : amdgpu_vm_unreserve_vmid(ctx->dev, 0);
+   return 0;
+}
+
+static int amdgpu_ccmd_set_pstate(struct amdgpu_context *ctx, const struct vdrm_ccmd_req *hdr)
+{
+   const struct amdgpu_ccmd_set_pstate_req *req = to_amdgpu_ccmd_set_pstate_req(hdr);
+   struct amdgpu_ccmd_set_pstate_rsp *rsp;
+   rsp = amdgpu_context_rsp(ctx, hdr, sizeof(*rsp));
+   amdgpu_context_handle actx = _mesa_hash_table_u64_search(ctx->id_to_ctx,
+                                                            (uintptr_t)req->ctx_id);
+   if (actx == NULL) {
+      print(0, "Couldn't find amdgpu_context with id %d", req->ctx_id);
+      rsp->hdr.ret = -EINVAL;
+      return -1;
+   }
+   rsp->hdr.ret = amdgpu_cs_ctx_stable_pstate(actx, req->op, req->flags, &rsp->out_flags);
+   return 0;
+}
+
+static const struct ccmd {
+   const char *name;
+   int (*handler)(struct amdgpu_context *ctx, const struct vdrm_ccmd_req *hdr);
+   size_t size;
+} ccmd_dispatch[] = {
+#define HANDLER(N, n)                                                                    \
+   [AMDGPU_CCMD_##N] = {#N, amdgpu_ccmd_##n, sizeof(struct amdgpu_ccmd_##n##_req)}
+   HANDLER(QUERY_INFO, query_info),
+   HANDLER(GEM_NEW, gem_new),
+   HANDLER(BO_VA_OP, bo_va_op),
+   HANDLER(CS_SUBMIT, cs_submit),
+   HANDLER(SET_METADATA, set_metadata),
+   HANDLER(BO_QUERY_INFO, bo_query_info),
+   HANDLER(CREATE_CTX, create_ctx),
+   HANDLER(RESERVE_VMID, reserve_vmid),
+   HANDLER(SET_PSTATE, set_pstate),
+};
+
+const char * __cmd_names[] = {
+   "QUERY_INFO   ",
+   "GEM_NEW      ",
+   "BO_VA_OP     ",
+   "CS_SUBMIT    ",
+   "SET_METADATA ",
+   "BO_QUERY_INFO",
+   "CREATE_CTX   ",
+   "RESERVE_VMID ",
+   "SET_PSTATE   ",
+};
+
+static int
+submit_cmd_dispatch(struct amdgpu_context *ctx, const struct vdrm_ccmd_req *hdr)
+{
+   int ret;
+
+   if (hdr->cmd >= ARRAY_SIZE(ccmd_dispatch)) {
+      print(0, "invalid cmd: %u", hdr->cmd);
+      return -EINVAL;
+   }
+
+   const struct ccmd *ccmd = &ccmd_dispatch[hdr->cmd];
+
+   if (!ccmd->handler) {
+      print(0, "no handler: %u", hdr->cmd);
+      return -EINVAL;
+   }
+
+   struct cmd_stat * const stat = &ctx->statistics.s[hdr->cmd - 1];
+   print(2, "command: %s (seqno:%u, sz:%zu)", __cmd_names[hdr->cmd - 1], hdr->seqno,
+         ccmd->size);
+
+   uint64_t start = util_current_thread_get_time_nano();
+
+   /* If the request length from the guest is smaller than the expected
+    * size, ie. newer host and older guest, we need to make a copy of
+    * the request with the new fields at the end zero initialized.
+    */
+   if (ccmd->size > hdr->len) {
+      uint8_t buf[ccmd->size];
+
+      memcpy(&buf[0], hdr, hdr->len);
+      memset(&buf[hdr->len], 0, ccmd->size - hdr->len);
+
+      ret = ccmd->handler(ctx, (struct vdrm_ccmd_req *)buf);
+   } else {
+      ret = ccmd->handler(ctx, hdr);
+   }
+
+   uint64_t end = util_current_thread_get_time_nano();
+
+   stat->count++;
+   uint64_t dt = (end - start) / 1000;
+   stat->min_duration_us = MIN2(stat->min_duration_us, dt);
+   stat->max_duration_us = MAX2(stat->max_duration_us, dt);
+   stat->total_duration_us += dt;
+
+   uint64_t ms = end / 1000000;
+   if (ms - ctx->statistics.last_print_ms >= 1000 && ctx->debug >= 1) {
+      unsigned c = (unsigned)((uintptr_t)ctx >> 8) % 256;
+      printf("\033[0;38;5;%dm", c);
+
+      uint64_t dt = ms - ctx->statistics.last_print_ms;
+      uint32_t total = 0;
+      int align = 1;
+      for (unsigned i = 0; i < ARRAY_SIZE(ctx->statistics.s); i++) {
+         total += ctx->statistics.s[i].count;
+         if (total >= 10000)
+            align = 5;
+         else if (total >= 1000)
+            align = 4;
+         else if (total >= 100)
+            align = 3;
+      }
+
+      printf("====== Stats for %s (%d cmd, dt: %ld ms) =====\n", ctx->debug_name, total, dt);
+      for (unsigned i = 0; i < ARRAY_SIZE(ctx->statistics.s); i++) {
+         if (ctx->statistics.s[i].count) {
+            int n = (int)roundf(100 * ctx->statistics.s[i].count / (float)total);
+            printf("\t%s %3d%% (n: %*d, min: %.3f ms, max: %.3f ms, avg: %.3f ms)\n",
+                   __cmd_names[i],
+                   n,
+                   align,
+                   ctx->statistics.s[i].count,
+                   ctx->statistics.s[i].min_duration_us / 1000.f,
+                   ctx->statistics.s[i].max_duration_us / 1000.f,
+                   ctx->statistics.s[i].total_duration_us / (ctx->statistics.s[i].count * 1000.f));
+         }
+      }
+      printf("\033[0m");
+      memset(ctx->statistics.s, 0, sizeof(ctx->statistics.s));
+      ctx->statistics.last_print_ms = ms;
+   }
+
+   if (ret)
+      print(0, "%s: dispatch failed: %d (%s)", ccmd->name, ret, strerror(errno));
+
+   /* If the response length from the guest is smaller than the
+    * expected size, ie. newer host and older guest, then a shadow
+    * copy is used, and we need to copy back to the actual rsp
+    * buffer.
+    */
+   struct amdgpu_ccmd_rsp *rsp = amdgpu_context_rsp_noshadow(ctx, hdr);
+   if (ctx->current_rsp && (ctx->current_rsp != rsp)) {
+      unsigned len = rsp->base.len;
+      memcpy(rsp, ctx->current_rsp, len);
+      rsp->base.len = len;
+      free(ctx->current_rsp);
+   }
+   ctx->current_rsp = NULL;
+
+   /* Note that commands with no response, like SET_DEBUGINFO, could
+    * be sent before the shmem buffer is allocated:
+    */
+   if (ctx->shmem) {
+      /* TODO better way to do this?  We need ACQ_REL semantics (AFAIU)
+       * to ensure that writes to response buffer are visible to the
+       * guest process before the update of the seqno.  Otherwise we
+       * could just use p_atomic_set.
+       */
+      uint32_t seqno = hdr->seqno;
+      p_atomic_xchg(&ctx->shmem->base.seqno, seqno);
+      print(3, "seqno: %u", seqno);
+   }
+
+   return ret;
+}
+
+static int
+amdgpu_renderer_submit_cmd(struct virgl_context *vctx, const void *_buffer, size_t size)
+{
+   struct amdgpu_context *ctx = to_amdgpu_context(vctx);
+   const uint8_t *buffer = _buffer;
+
+   while (size >= sizeof(struct vdrm_ccmd_req)) {
+      const struct vdrm_ccmd_req *hdr = (const struct vdrm_ccmd_req *)buffer;
+
+      /* Sanity check first: */
+      if ((hdr->len > size) || (hdr->len < sizeof(*hdr)) || (hdr->len % 4)) {
+         print(0, "bad size, %u vs %zu (%u)", hdr->len, size, hdr->cmd);
+         return -EINVAL;
+      }
+
+      if (hdr->rsp_off % 4) {
+         print(0, "bad rsp_off, %u", hdr->rsp_off);
+         return -EINVAL;
+      }
+
+      int ret = submit_cmd_dispatch(ctx, hdr);
+      if (ret) {
+         print(0, "dispatch failed: %d (%u)", ret, hdr->cmd);
+         return ret;
+      }
+
+      buffer += hdr->len;
+      size -= hdr->len;
+   }
+
+   if (size > 0) {
+      print(0, "bad size, %zu trailing bytes", size);
+      return -EINVAL;
+   }
+
+   return 0;
+}
+
+static int
+amdgpu_renderer_get_fencing_fd(struct virgl_context *vctx)
+{
+   assert(false); /* Unused ? */
+   struct amdgpu_context *mctx = to_amdgpu_context(vctx);
+   return mctx->eventfd;
+}
+
+static void
+amdgpu_renderer_retire_fences(UNUSED struct virgl_context *vctx)
+{
+   /* No-op as VIRGL_RENDERER_ASYNC_FENCE_CB is required */
+}
+
+static int
+amdgpu_renderer_submit_fence(struct virgl_context *vctx, uint32_t flags,
+                             uint32_t ring_idx, uint64_t fence_id)
+{
+   struct amdgpu_context *ctx = to_amdgpu_context(vctx);
+
+   /* timeline is ring_idx-1 (because ring_idx 0 is host CPU timeline) */
+   if (ring_idx > AMDGPU_HW_IP_NUM) {
+      print(0, "invalid ring_idx: %" PRIu32, ring_idx);
+      return -EINVAL;
+   }
+   /* ring_idx zero is used for the guest to synchronize with host CPU,
+    * meaning by the time ->submit_fence() is called, the fence has
+    * already passed.. so just immediate signal:
+    */
+   if (ring_idx == 0) {
+      vctx->fence_retire(vctx, ring_idx, fence_id);
+      return 0;
+   }
+
+   print(3, "ring_idx: %d fence_id: %lu", ring_idx, fence_id);
+   return drm_timeline_submit_fence(&ctx->timelines[ring_idx - 1], flags, fence_id);
+}
+
+static void
+amdgpu_renderer_fence_retire(struct virgl_context *vctx,
+                             uint32_t ring_idx,
+                             uint64_t fence_id)
+{
+   struct amdgpu_context *ctx = to_amdgpu_context(vctx);
+   print(3, "ring_idx: %d fence_id: %lu", ring_idx, fence_id);
+   vctx->fence_retire(vctx, ring_idx, fence_id);
+}
+
+struct virgl_context *
+amdgpu_renderer_create(int fd, size_t debug_len, const char *debug_name)
+{
+   struct amdgpu_context *ctx;
+   amdgpu_device_handle dev;
+
+   int r;
+   uint32_t drm_major, drm_minor;
+
+
+   /* Don't use libdrm_amdgpu device deduplication logic. The goal is to
+    * get a different drm_file for each application inside the guest so we
+    * get inter-application implicit synchronisation handled for us by
+    * the kernel.
+    * This also makes the application completely separate (eg: each one gets
+    * its own VM space).
+    * Using dlopen wouldn't work because it returns the same refcounted handle.
+    * dlmopen(LM_ID_NEWLM, ...) would work but there can only be 16 namespaces.
+    */
+   r = amdgpu_device_initialize2(fd, false, &drm_major, &drm_minor, &dev);
+   if (r) {
+      printf("amdgpu_device_initialize failed (fd=%d, %s)\n", fd, strerror(errno));
+      close(fd);
+      return NULL;
+   }
+
+   uint32_t timeline_count = 0;
+   for (unsigned ip_type = 0; ip_type < AMDGPU_HW_IP_NUM; ip_type++) {
+      struct drm_amdgpu_info_hw_ip ip_info = {0};
+
+      int r = amdgpu_query_hw_ip_info(dev, ip_type, 0, &ip_info);
+      if (r < 0)
+         continue;
+
+      if (ip_info.available_rings)
+         timeline_count += util_bitcount(ip_info.available_rings);
+   }
+   if (timeline_count == 0) {
+      printf("ERR: No available_rings for dev %d\n", fd);
+      amdgpu_device_deinitialize(dev);
+      return NULL;
+   }
+
+   ctx = calloc(1, sizeof(struct amdgpu_context) + timeline_count * sizeof(ctx->timelines[0]));
+   ctx->debug_name = strndup(debug_name, debug_len);
+   ctx->debug = -1;
+   ctx->dev = dev;
+   const char *d = getenv("DEBUG");
+   if (d)
+      ctx->debug = atoi(d);
+
+   print(1, "amdgpu_renderer_create name=%s fd=%d (from %d) -> dev=%p", ctx->debug_name, fd,
+         amdgpu_device_get_fd(ctx->dev), (void*)ctx->dev);
+
+   /* Indexed by blob_id, but only lower 32b of blob_id are used: */
+   ctx->blob_table = _mesa_hash_table_create_u32_keys(NULL);
+   /* Indexed by res_id: */
+   ctx->resource_table = _mesa_hash_table_create_u32_keys(NULL);
+
+   ctx->base.destroy = amdgpu_renderer_destroy;
+   ctx->base.attach_resource = amdgpu_renderer_attach_resource;
+   ctx->base.detach_resource = amdgpu_renderer_detach_resource;
+   ctx->base.export_opaque_handle = amdgpu_renderer_export_opaque_handle;
+   ctx->base.transfer_3d = amdgpu_renderer_transfer_3d;
+   ctx->base.get_blob = amdgpu_renderer_get_blob;
+   ctx->base.submit_cmd = amdgpu_renderer_submit_cmd;
+   ctx->base.get_fencing_fd = amdgpu_renderer_get_fencing_fd;
+   ctx->base.retire_fences = amdgpu_renderer_retire_fences;
+   ctx->base.submit_fence = amdgpu_renderer_submit_fence;
+   ctx->base.supports_fence_sharing = true;
+
+   ctx->id_to_ctx = _mesa_hash_table_u64_create(NULL);
+
+   ctx->eventfd = create_eventfd(0);
+
+   /* Ring 0 is for CPU execution. */
+   uint32_t ring_idx = 1;
+
+   /* TODO: add a setting to control which IP are exposed to the
+    * guests.
+    */
+   for (unsigned ip_type = 0; ip_type < AMDGPU_HW_IP_NUM; ip_type++) {
+      struct drm_amdgpu_info_hw_ip ip_info = {0};
+
+      int r = amdgpu_query_hw_ip_info(dev, ip_type, 0, &ip_info);
+      if (r < 0)
+         continue;
+
+      int cnt = util_bitcount(ip_info.available_rings);
+      for (int i = 0; i < cnt; i++) {
+         char *name;
+         asprintf(&name, "a-%s-%d", ctx->debug_name, ring_idx);
+         drm_timeline_init(&ctx->timelines[ring_idx - 1], &ctx->base,
+                           name,
+                           ctx->eventfd, ring_idx,
+                           amdgpu_renderer_fence_retire);
+         ring_idx += 1;
+      }
+   }
+   ctx->timeline_count = timeline_count;
+   assert(ring_idx == timeline_count + 1);
+
+   ctx->statistics.last_print_ms = util_current_thread_get_time_nano() / 1000000;
+
+   close(fd);
+
+   return &ctx->base;
+}
diff --git a/src/drm/amdgpu/amdgpu_renderer.h b/src/drm/amdgpu/amdgpu_renderer.h
new file mode 100644
index 00000000..40ebb1fc
--- /dev/null
+++ b/src/drm/amdgpu/amdgpu_renderer.h
@@ -0,0 +1,25 @@
+/*
+ * Copyright 2023 Advanced Micro Devices, Inc
+ * SPDX-License-Identifier: MIT
+ */
+
+#ifndef AMDGPU_RENDERER_H_
+#define AMDGPU_RENDERER_H_
+
+#include "config.h"
+
+#include <inttypes.h>
+#include <stddef.h>
+#include <stdint.h>
+#include <time.h>
+
+#include "pipe/p_defines.h"
+
+#include "amdgpu_drm.h"
+#include "drm_hw.h"
+
+int amdgpu_renderer_probe(int fd, struct virgl_renderer_capset_drm *capset);
+
+struct virgl_context *amdgpu_renderer_create(int fd, size_t debug_len, const char *debug_name);
+
+#endif /* AMDGPU_RENDERER_H_ */
diff --git a/src/drm/amdgpu/amdgpu_virtio_proto.h b/src/drm/amdgpu/amdgpu_virtio_proto.h
new file mode 100644
index 00000000..39add7d2
--- /dev/null
+++ b/src/drm/amdgpu/amdgpu_virtio_proto.h
@@ -0,0 +1,205 @@
+#ifndef AMDGPU_VIRTIO_PROTO_H
+#define AMDGPU_VIRTIO_PROTO_H
+
+#include <stdint.h>
+#include "amdgpu.h"
+#include "amdgpu_drm.h"
+
+enum amdgpu_ccmd {
+   AMDGPU_CCMD_QUERY_INFO = 1,
+   AMDGPU_CCMD_GEM_NEW,
+   AMDGPU_CCMD_BO_VA_OP,
+   AMDGPU_CCMD_CS_SUBMIT,
+   AMDGPU_CCMD_SET_METADATA,
+   AMDGPU_CCMD_BO_QUERY_INFO,
+   AMDGPU_CCMD_CREATE_CTX,
+   AMDGPU_CCMD_RESERVE_VMID,
+   AMDGPU_CCMD_SET_PSTATE,
+};
+
+struct amdgpu_ccmd_rsp {
+   struct vdrm_ccmd_rsp base;
+   int32_t ret;
+};
+
+
+/**
+ * Defines the layout of shmem buffer used for host->guest communication.
+ */
+struct amdvgpu_shmem {
+   struct vdrm_shmem base;
+
+   /**
+    * Counter that is incremented on asynchronous errors, like SUBMIT
+    * or GEM_NEW failures.  The guest should treat errors as context-
+    * lost.
+    */
+   uint32_t async_error;
+
+   uint32_t __pad;
+
+   struct amdgpu_heap_info gtt;
+   struct amdgpu_heap_info vram;
+   struct amdgpu_heap_info vis_vram;
+};
+DEFINE_CAST(vdrm_shmem, amdvgpu_shmem)
+
+
+#define AMDGPU_CCMD(_cmd, _len) (struct vdrm_ccmd_req){ \
+       .cmd = AMDGPU_CCMD_##_cmd,                         \
+       .len = (_len),                                     \
+   }
+
+/*
+ * AMDGPU_CCMD_QUERY_INFO
+ *
+ * This is amdgpu_query_info.
+ */
+struct amdgpu_ccmd_query_info_req {
+   struct vdrm_ccmd_req hdr;
+   struct drm_amdgpu_info info;
+};
+DEFINE_CAST(vdrm_ccmd_req, amdgpu_ccmd_query_info_req)
+
+struct amdgpu_ccmd_query_info_rsp {
+   struct amdgpu_ccmd_rsp hdr;
+   uint8_t payload[];
+};
+
+struct amdgpu_ccmd_gem_new_req {
+   struct vdrm_ccmd_req hdr;
+
+   uint64_t blob_id;
+   uint64_t va;
+   uint32_t pad;
+   uint32_t vm_flags;
+   uint64_t vm_map_size; /* may be smaller than alloc_size */
+
+   /* This is amdgpu_bo_alloc_request but padded correctly. */
+   struct {
+      uint64_t alloc_size;
+      uint64_t phys_alignment;
+      uint32_t preferred_heap;
+      uint32_t __pad;
+      uint64_t flags;
+   } r;
+};
+DEFINE_CAST(vdrm_ccmd_req, amdgpu_ccmd_gem_new_req)
+
+
+/*
+ * AMDGPU_CCMD_BO_VA_OP
+ *
+ */
+struct amdgpu_ccmd_bo_va_op_req {
+   struct vdrm_ccmd_req hdr;
+   uint64_t va;
+   uint64_t vm_map_size;
+   uint64_t flags;
+   uint64_t offset;
+   uint32_t res_id;
+   uint32_t op;
+   bool is_sparse_bo;
+};
+DEFINE_CAST(vdrm_ccmd_req, amdgpu_ccmd_bo_va_op_req)
+
+/*
+ * AMDGPU_CCMD_CS_SUBMIT
+ */
+struct amdgpu_ccmd_cs_submit_req {
+   struct vdrm_ccmd_req hdr;
+
+   uint32_t ctx_id;
+   uint32_t num_chunks;
+   uint32_t bo_number;
+   uint32_t ring_idx;
+
+   /* Starts with a descriptor array:
+    *     (chunk_id, offset_in_payload), ...
+    */
+   uint8_t payload[];
+};
+DEFINE_CAST(vdrm_ccmd_req, amdgpu_ccmd_cs_submit_req)
+
+/*
+ * AMDGPU_CCMD_SET_METADATA
+ */
+struct amdgpu_ccmd_set_metadata_req {
+   struct vdrm_ccmd_req hdr;
+   uint64_t flags;
+   uint64_t tiling_info;
+   uint32_t res_id;
+   uint32_t size_metadata;
+   uint32_t umd_metadata[];
+};
+DEFINE_CAST(vdrm_ccmd_req, amdgpu_ccmd_set_metadata_req)
+
+
+/*
+ * AMDGPU_CCMD_BO_QUERY_INFO
+ */
+struct amdgpu_ccmd_bo_query_info_req {
+   struct vdrm_ccmd_req hdr;
+   uint32_t res_id;
+};
+DEFINE_CAST(vdrm_ccmd_req, amdgpu_ccmd_bo_query_info_req)
+
+struct amdgpu_ccmd_bo_query_info_rsp {
+   struct amdgpu_ccmd_rsp hdr;
+   uint32_t __pad;
+   /* This is almost struct amdgpu_bo_info, but padded to get
+    * the same struct on 32 bit and 64 bit builds.
+    */
+   struct {
+      uint64_t                   alloc_size;           /*     0     8 */
+      uint64_t                   phys_alignment;       /*     8     8 */
+      uint32_t                   preferred_heap;       /*    16     4 */
+      uint32_t __pad;
+      uint64_t                   alloc_flags;          /*    20     8 */
+      struct amdgpu_bo_metadata  metadata;
+   } info;
+};
+
+/*
+ * AMDGPU_CCMD_CREATE_CTX
+ */
+struct amdgpu_ccmd_create_ctx_req {
+   struct vdrm_ccmd_req hdr;
+   union {
+      int32_t priority; /* create */
+      uint32_t id;      /* destroy */
+   };
+   bool create;
+};
+DEFINE_CAST(vdrm_ccmd_req, amdgpu_ccmd_create_ctx_req)
+
+struct amdgpu_ccmd_create_ctx_rsp {
+   struct amdgpu_ccmd_rsp hdr;
+   uint32_t ctx_id;
+};
+
+/*
+ * AMDGPU_CCMD_RESERVE_VMID
+ */
+struct amdgpu_ccmd_reserve_vmid_req {
+   struct vdrm_ccmd_req hdr;
+   bool enable;
+};
+DEFINE_CAST(vdrm_ccmd_req, amdgpu_ccmd_reserve_vmid_req)
+
+/*
+ * AMDGPU_CCMD_SET_PSTATE
+ */
+struct amdgpu_ccmd_set_pstate_req {
+   struct vdrm_ccmd_req hdr;
+   uint32_t ctx_id;
+   uint32_t op;
+   uint32_t flags;
+};
+struct amdgpu_ccmd_set_pstate_rsp {
+   struct amdgpu_ccmd_rsp hdr;
+   uint32_t out_flags;
+};
+DEFINE_CAST(vdrm_ccmd_req, amdgpu_ccmd_set_pstate_req)
+
+#endif
diff --git a/src/drm/drm-uapi/amdgpu_drm.h b/src/drm/drm-uapi/amdgpu_drm.h
new file mode 100644
index 00000000..4038abe8
--- /dev/null
+++ b/src/drm/drm-uapi/amdgpu_drm.h
@@ -0,0 +1,1200 @@
+/* amdgpu_drm.h -- Public header for the amdgpu driver -*- linux-c -*-
+ *
+ * Copyright 2000 Precision Insight, Inc., Cedar Park, Texas.
+ * Copyright 2000 VA Linux Systems, Inc., Fremont, California.
+ * Copyright 2002 Tungsten Graphics, Inc., Cedar Park, Texas.
+ * Copyright 2014 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *    Kevin E. Martin <martin@valinux.com>
+ *    Gareth Hughes <gareth@valinux.com>
+ *    Keith Whitwell <keith@tungstengraphics.com>
+ */
+
+#ifndef __AMDGPU_DRM_H__
+#define __AMDGPU_DRM_H__
+
+#include "drm.h"
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+#define DRM_AMDGPU_GEM_CREATE		0x00
+#define DRM_AMDGPU_GEM_MMAP		0x01
+#define DRM_AMDGPU_CTX			0x02
+#define DRM_AMDGPU_BO_LIST		0x03
+#define DRM_AMDGPU_CS			0x04
+#define DRM_AMDGPU_INFO			0x05
+#define DRM_AMDGPU_GEM_METADATA		0x06
+#define DRM_AMDGPU_GEM_WAIT_IDLE	0x07
+#define DRM_AMDGPU_GEM_VA		0x08
+#define DRM_AMDGPU_WAIT_CS		0x09
+#define DRM_AMDGPU_GEM_OP		0x10
+#define DRM_AMDGPU_GEM_USERPTR		0x11
+#define DRM_AMDGPU_WAIT_FENCES		0x12
+#define DRM_AMDGPU_VM			0x13
+#define DRM_AMDGPU_FENCE_TO_HANDLE	0x14
+#define DRM_AMDGPU_SCHED		0x15
+
+#define DRM_IOCTL_AMDGPU_GEM_CREATE	DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_GEM_CREATE, union drm_amdgpu_gem_create)
+#define DRM_IOCTL_AMDGPU_GEM_MMAP	DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_GEM_MMAP, union drm_amdgpu_gem_mmap)
+#define DRM_IOCTL_AMDGPU_CTX		DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_CTX, union drm_amdgpu_ctx)
+#define DRM_IOCTL_AMDGPU_BO_LIST	DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_BO_LIST, union drm_amdgpu_bo_list)
+#define DRM_IOCTL_AMDGPU_CS		DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_CS, union drm_amdgpu_cs)
+#define DRM_IOCTL_AMDGPU_INFO		DRM_IOW(DRM_COMMAND_BASE + DRM_AMDGPU_INFO, struct drm_amdgpu_info)
+#define DRM_IOCTL_AMDGPU_GEM_METADATA	DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_GEM_METADATA, struct drm_amdgpu_gem_metadata)
+#define DRM_IOCTL_AMDGPU_GEM_WAIT_IDLE	DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_GEM_WAIT_IDLE, union drm_amdgpu_gem_wait_idle)
+#define DRM_IOCTL_AMDGPU_GEM_VA		DRM_IOW(DRM_COMMAND_BASE + DRM_AMDGPU_GEM_VA, struct drm_amdgpu_gem_va)
+#define DRM_IOCTL_AMDGPU_WAIT_CS	DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_WAIT_CS, union drm_amdgpu_wait_cs)
+#define DRM_IOCTL_AMDGPU_GEM_OP		DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_GEM_OP, struct drm_amdgpu_gem_op)
+#define DRM_IOCTL_AMDGPU_GEM_USERPTR	DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_GEM_USERPTR, struct drm_amdgpu_gem_userptr)
+#define DRM_IOCTL_AMDGPU_WAIT_FENCES	DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_WAIT_FENCES, union drm_amdgpu_wait_fences)
+#define DRM_IOCTL_AMDGPU_VM		DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_VM, union drm_amdgpu_vm)
+#define DRM_IOCTL_AMDGPU_FENCE_TO_HANDLE DRM_IOWR(DRM_COMMAND_BASE + DRM_AMDGPU_FENCE_TO_HANDLE, union drm_amdgpu_fence_to_handle)
+#define DRM_IOCTL_AMDGPU_SCHED		DRM_IOW(DRM_COMMAND_BASE + DRM_AMDGPU_SCHED, union drm_amdgpu_sched)
+
+/**
+ * DOC: memory domains
+ *
+ * %AMDGPU_GEM_DOMAIN_CPU	System memory that is not GPU accessible.
+ * Memory in this pool could be swapped out to disk if there is pressure.
+ *
+ * %AMDGPU_GEM_DOMAIN_GTT	GPU accessible system memory, mapped into the
+ * GPU's virtual address space via gart. Gart memory linearizes non-contiguous
+ * pages of system memory, allows GPU access system memory in a linearized
+ * fashion.
+ *
+ * %AMDGPU_GEM_DOMAIN_VRAM	Local video memory. For APUs, it is memory
+ * carved out by the BIOS.
+ *
+ * %AMDGPU_GEM_DOMAIN_GDS	Global on-chip data storage used to share data
+ * across shader threads.
+ *
+ * %AMDGPU_GEM_DOMAIN_GWS	Global wave sync, used to synchronize the
+ * execution of all the waves on a device.
+ *
+ * %AMDGPU_GEM_DOMAIN_OA	Ordered append, used by 3D or Compute engines
+ * for appending data.
+ */
+#define AMDGPU_GEM_DOMAIN_CPU		0x1
+#define AMDGPU_GEM_DOMAIN_GTT		0x2
+#define AMDGPU_GEM_DOMAIN_VRAM		0x4
+#define AMDGPU_GEM_DOMAIN_GDS		0x8
+#define AMDGPU_GEM_DOMAIN_GWS		0x10
+#define AMDGPU_GEM_DOMAIN_OA		0x20
+#define AMDGPU_GEM_DOMAIN_MASK		(AMDGPU_GEM_DOMAIN_CPU | \
+					 AMDGPU_GEM_DOMAIN_GTT | \
+					 AMDGPU_GEM_DOMAIN_VRAM | \
+					 AMDGPU_GEM_DOMAIN_GDS | \
+					 AMDGPU_GEM_DOMAIN_GWS | \
+					 AMDGPU_GEM_DOMAIN_OA)
+
+/* Flag that CPU access will be required for the case of VRAM domain */
+#define AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED	(1 << 0)
+/* Flag that CPU access will not work, this VRAM domain is invisible */
+#define AMDGPU_GEM_CREATE_NO_CPU_ACCESS		(1 << 1)
+/* Flag that USWC attributes should be used for GTT */
+#define AMDGPU_GEM_CREATE_CPU_GTT_USWC		(1 << 2)
+/* Flag that the memory should be in VRAM and cleared */
+#define AMDGPU_GEM_CREATE_VRAM_CLEARED		(1 << 3)
+/* Flag that allocating the BO should use linear VRAM */
+#define AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS	(1 << 5)
+/* Flag that BO is always valid in this VM */
+#define AMDGPU_GEM_CREATE_VM_ALWAYS_VALID	(1 << 6)
+/* Flag that BO sharing will be explicitly synchronized */
+#define AMDGPU_GEM_CREATE_EXPLICIT_SYNC		(1 << 7)
+/* Flag that indicates allocating MQD gart on GFX9, where the mtype
+ * for the second page onward should be set to NC. It should never
+ * be used by user space applications.
+ */
+#define AMDGPU_GEM_CREATE_CP_MQD_GFX9		(1 << 8)
+/* Flag that BO may contain sensitive data that must be wiped before
+ * releasing the memory
+ */
+#define AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE	(1 << 9)
+/* Flag that BO will be encrypted and that the TMZ bit should be
+ * set in the PTEs when mapping this buffer via GPUVM or
+ * accessing it with various hw blocks
+ */
+#define AMDGPU_GEM_CREATE_ENCRYPTED		(1 << 10)
+/* Flag that BO will be used only in preemptible context, which does
+ * not require GTT memory accounting
+ */
+#define AMDGPU_GEM_CREATE_PREEMPTIBLE		(1 << 11)
+/* Flag that BO can be discarded under memory pressure without keeping the
+ * content.
+ */
+#define AMDGPU_GEM_CREATE_DISCARDABLE		(1 << 12)
+/* Flag that BO is shared coherently between multiple devices or CPU threads.
+ * May depend on GPU instructions to flush caches explicitly
+ *
+ * This influences the choice of MTYPE in the PTEs on GFXv9 and later GPUs and
+ * may override the MTYPE selected in AMDGPU_VA_OP_MAP.
+ */
+#define AMDGPU_GEM_CREATE_COHERENT		(1 << 13)
+/* Flag that BO should not be cached by GPU. Coherent without having to flush
+ * GPU caches explicitly
+ *
+ * This influences the choice of MTYPE in the PTEs on GFXv9 and later GPUs and
+ * may override the MTYPE selected in AMDGPU_VA_OP_MAP.
+ */
+#define AMDGPU_GEM_CREATE_UNCACHED		(1 << 14)
+
+struct drm_amdgpu_gem_create_in  {
+	/** the requested memory size */
+	__u64 bo_size;
+	/** physical start_addr alignment in bytes for some HW requirements */
+	__u64 alignment;
+	/** the requested memory domains */
+	__u64 domains;
+	/** allocation flags */
+	__u64 domain_flags;
+};
+
+struct drm_amdgpu_gem_create_out  {
+	/** returned GEM object handle */
+	__u32 handle;
+	__u32 _pad;
+};
+
+union drm_amdgpu_gem_create {
+	struct drm_amdgpu_gem_create_in		in;
+	struct drm_amdgpu_gem_create_out	out;
+};
+
+/** Opcode to create new residency list.  */
+#define AMDGPU_BO_LIST_OP_CREATE	0
+/** Opcode to destroy previously created residency list */
+#define AMDGPU_BO_LIST_OP_DESTROY	1
+/** Opcode to update resource information in the list */
+#define AMDGPU_BO_LIST_OP_UPDATE	2
+
+struct drm_amdgpu_bo_list_in {
+	/** Type of operation */
+	__u32 operation;
+	/** Handle of list or 0 if we want to create one */
+	__u32 list_handle;
+	/** Number of BOs in list  */
+	__u32 bo_number;
+	/** Size of each element describing BO */
+	__u32 bo_info_size;
+	/** Pointer to array describing BOs */
+	__u64 bo_info_ptr;
+};
+
+struct drm_amdgpu_bo_list_entry {
+	/** Handle of BO */
+	__u32 bo_handle;
+	/** New (if specified) BO priority to be used during migration */
+	__u32 bo_priority;
+};
+
+struct drm_amdgpu_bo_list_out {
+	/** Handle of resource list  */
+	__u32 list_handle;
+	__u32 _pad;
+};
+
+union drm_amdgpu_bo_list {
+	struct drm_amdgpu_bo_list_in in;
+	struct drm_amdgpu_bo_list_out out;
+};
+
+/* context related */
+#define AMDGPU_CTX_OP_ALLOC_CTX	1
+#define AMDGPU_CTX_OP_FREE_CTX	2
+#define AMDGPU_CTX_OP_QUERY_STATE	3
+#define AMDGPU_CTX_OP_QUERY_STATE2	4
+#define AMDGPU_CTX_OP_GET_STABLE_PSTATE	5
+#define AMDGPU_CTX_OP_SET_STABLE_PSTATE	6
+
+/* GPU reset status */
+#define AMDGPU_CTX_NO_RESET		0
+/* this the context caused it */
+#define AMDGPU_CTX_GUILTY_RESET		1
+/* some other context caused it */
+#define AMDGPU_CTX_INNOCENT_RESET	2
+/* unknown cause */
+#define AMDGPU_CTX_UNKNOWN_RESET	3
+
+/* indicate gpu reset occured after ctx created */
+#define AMDGPU_CTX_QUERY2_FLAGS_RESET    (1<<0)
+/* indicate vram lost occured after ctx created */
+#define AMDGPU_CTX_QUERY2_FLAGS_VRAMLOST (1<<1)
+/* indicate some job from this context once cause gpu hang */
+#define AMDGPU_CTX_QUERY2_FLAGS_GUILTY   (1<<2)
+/* indicate some errors are detected by RAS */
+#define AMDGPU_CTX_QUERY2_FLAGS_RAS_CE   (1<<3)
+#define AMDGPU_CTX_QUERY2_FLAGS_RAS_UE   (1<<4)
+
+/* Context priority level */
+#define AMDGPU_CTX_PRIORITY_UNSET       -2048
+#define AMDGPU_CTX_PRIORITY_VERY_LOW    -1023
+#define AMDGPU_CTX_PRIORITY_LOW         -512
+#define AMDGPU_CTX_PRIORITY_NORMAL      0
+/*
+ * When used in struct drm_amdgpu_ctx_in, a priority above NORMAL requires
+ * CAP_SYS_NICE or DRM_MASTER
+*/
+#define AMDGPU_CTX_PRIORITY_HIGH        512
+#define AMDGPU_CTX_PRIORITY_VERY_HIGH   1023
+
+/* select a stable profiling pstate for perfmon tools */
+#define AMDGPU_CTX_STABLE_PSTATE_FLAGS_MASK  0xf
+#define AMDGPU_CTX_STABLE_PSTATE_NONE  0
+#define AMDGPU_CTX_STABLE_PSTATE_STANDARD  1
+#define AMDGPU_CTX_STABLE_PSTATE_MIN_SCLK  2
+#define AMDGPU_CTX_STABLE_PSTATE_MIN_MCLK  3
+#define AMDGPU_CTX_STABLE_PSTATE_PEAK  4
+
+struct drm_amdgpu_ctx_in {
+	/** AMDGPU_CTX_OP_* */
+	__u32	op;
+	/** Flags */
+	__u32	flags;
+	__u32	ctx_id;
+	/** AMDGPU_CTX_PRIORITY_* */
+	__s32	priority;
+};
+
+union drm_amdgpu_ctx_out {
+		struct {
+			__u32	ctx_id;
+			__u32	_pad;
+		} alloc;
+
+		struct {
+			/** For future use, no flags defined so far */
+			__u64	flags;
+			/** Number of resets caused by this context so far. */
+			__u32	hangs;
+			/** Reset status since the last call of the ioctl. */
+			__u32	reset_status;
+		} state;
+
+		struct {
+			__u32	flags;
+			__u32	_pad;
+		} pstate;
+};
+
+union drm_amdgpu_ctx {
+	struct drm_amdgpu_ctx_in in;
+	union drm_amdgpu_ctx_out out;
+};
+
+/* vm ioctl */
+#define AMDGPU_VM_OP_RESERVE_VMID	1
+#define AMDGPU_VM_OP_UNRESERVE_VMID	2
+
+struct drm_amdgpu_vm_in {
+	/** AMDGPU_VM_OP_* */
+	__u32	op;
+	__u32	flags;
+};
+
+struct drm_amdgpu_vm_out {
+	/** For future use, no flags defined so far */
+	__u64	flags;
+};
+
+union drm_amdgpu_vm {
+	struct drm_amdgpu_vm_in in;
+	struct drm_amdgpu_vm_out out;
+};
+
+/* sched ioctl */
+#define AMDGPU_SCHED_OP_PROCESS_PRIORITY_OVERRIDE	1
+#define AMDGPU_SCHED_OP_CONTEXT_PRIORITY_OVERRIDE	2
+
+struct drm_amdgpu_sched_in {
+	/* AMDGPU_SCHED_OP_* */
+	__u32	op;
+	__u32	fd;
+	/** AMDGPU_CTX_PRIORITY_* */
+	__s32	priority;
+	__u32   ctx_id;
+};
+
+union drm_amdgpu_sched {
+	struct drm_amdgpu_sched_in in;
+};
+
+/*
+ * This is not a reliable API and you should expect it to fail for any
+ * number of reasons and have fallback path that do not use userptr to
+ * perform any operation.
+ */
+#define AMDGPU_GEM_USERPTR_READONLY	(1 << 0)
+#define AMDGPU_GEM_USERPTR_ANONONLY	(1 << 1)
+#define AMDGPU_GEM_USERPTR_VALIDATE	(1 << 2)
+#define AMDGPU_GEM_USERPTR_REGISTER	(1 << 3)
+
+struct drm_amdgpu_gem_userptr {
+	__u64		addr;
+	__u64		size;
+	/* AMDGPU_GEM_USERPTR_* */
+	__u32		flags;
+	/* Resulting GEM handle */
+	__u32		handle;
+};
+
+/* SI-CI-VI: */
+/* same meaning as the GB_TILE_MODE and GL_MACRO_TILE_MODE fields */
+#define AMDGPU_TILING_ARRAY_MODE_SHIFT			0
+#define AMDGPU_TILING_ARRAY_MODE_MASK			0xf
+#define AMDGPU_TILING_PIPE_CONFIG_SHIFT			4
+#define AMDGPU_TILING_PIPE_CONFIG_MASK			0x1f
+#define AMDGPU_TILING_TILE_SPLIT_SHIFT			9
+#define AMDGPU_TILING_TILE_SPLIT_MASK			0x7
+#define AMDGPU_TILING_MICRO_TILE_MODE_SHIFT		12
+#define AMDGPU_TILING_MICRO_TILE_MODE_MASK		0x7
+#define AMDGPU_TILING_BANK_WIDTH_SHIFT			15
+#define AMDGPU_TILING_BANK_WIDTH_MASK			0x3
+#define AMDGPU_TILING_BANK_HEIGHT_SHIFT			17
+#define AMDGPU_TILING_BANK_HEIGHT_MASK			0x3
+#define AMDGPU_TILING_MACRO_TILE_ASPECT_SHIFT		19
+#define AMDGPU_TILING_MACRO_TILE_ASPECT_MASK		0x3
+#define AMDGPU_TILING_NUM_BANKS_SHIFT			21
+#define AMDGPU_TILING_NUM_BANKS_MASK			0x3
+
+/* GFX9 and later: */
+#define AMDGPU_TILING_SWIZZLE_MODE_SHIFT		0
+#define AMDGPU_TILING_SWIZZLE_MODE_MASK			0x1f
+#define AMDGPU_TILING_DCC_OFFSET_256B_SHIFT		5
+#define AMDGPU_TILING_DCC_OFFSET_256B_MASK		0xFFFFFF
+#define AMDGPU_TILING_DCC_PITCH_MAX_SHIFT		29
+#define AMDGPU_TILING_DCC_PITCH_MAX_MASK		0x3FFF
+#define AMDGPU_TILING_DCC_INDEPENDENT_64B_SHIFT		43
+#define AMDGPU_TILING_DCC_INDEPENDENT_64B_MASK		0x1
+#define AMDGPU_TILING_DCC_INDEPENDENT_128B_SHIFT	44
+#define AMDGPU_TILING_DCC_INDEPENDENT_128B_MASK		0x1
+#define AMDGPU_TILING_SCANOUT_SHIFT			63
+#define AMDGPU_TILING_SCANOUT_MASK			0x1
+
+/* Set/Get helpers for tiling flags. */
+#define AMDGPU_TILING_SET(field, value) \
+	(((__u64)(value) & AMDGPU_TILING_##field##_MASK) << AMDGPU_TILING_##field##_SHIFT)
+#define AMDGPU_TILING_GET(value, field) \
+	(((__u64)(value) >> AMDGPU_TILING_##field##_SHIFT) & AMDGPU_TILING_##field##_MASK)
+
+#define AMDGPU_GEM_METADATA_OP_SET_METADATA                  1
+#define AMDGPU_GEM_METADATA_OP_GET_METADATA                  2
+
+/** The same structure is shared for input/output */
+struct drm_amdgpu_gem_metadata {
+	/** GEM Object handle */
+	__u32	handle;
+	/** Do we want get or set metadata */
+	__u32	op;
+	struct {
+		/** For future use, no flags defined so far */
+		__u64	flags;
+		/** family specific tiling info */
+		__u64	tiling_info;
+		__u32	data_size_bytes;
+		__u32	data[64];
+	} data;
+};
+
+struct drm_amdgpu_gem_mmap_in {
+	/** the GEM object handle */
+	__u32 handle;
+	__u32 _pad;
+};
+
+struct drm_amdgpu_gem_mmap_out {
+	/** mmap offset from the vma offset manager */
+	__u64 addr_ptr;
+};
+
+union drm_amdgpu_gem_mmap {
+	struct drm_amdgpu_gem_mmap_in   in;
+	struct drm_amdgpu_gem_mmap_out out;
+};
+
+struct drm_amdgpu_gem_wait_idle_in {
+	/** GEM object handle */
+	__u32 handle;
+	/** For future use, no flags defined so far */
+	__u32 flags;
+	/** Absolute timeout to wait */
+	__u64 timeout;
+};
+
+struct drm_amdgpu_gem_wait_idle_out {
+	/** BO status:  0 - BO is idle, 1 - BO is busy */
+	__u32 status;
+	/** Returned current memory domain */
+	__u32 domain;
+};
+
+union drm_amdgpu_gem_wait_idle {
+	struct drm_amdgpu_gem_wait_idle_in  in;
+	struct drm_amdgpu_gem_wait_idle_out out;
+};
+
+struct drm_amdgpu_wait_cs_in {
+	/* Command submission handle
+         * handle equals 0 means none to wait for
+         * handle equals ~0ull means wait for the latest sequence number
+         */
+	__u64 handle;
+	/** Absolute timeout to wait */
+	__u64 timeout;
+	__u32 ip_type;
+	__u32 ip_instance;
+	__u32 ring;
+	__u32 ctx_id;
+};
+
+struct drm_amdgpu_wait_cs_out {
+	/** CS status:  0 - CS completed, 1 - CS still busy */
+	__u64 status;
+};
+
+union drm_amdgpu_wait_cs {
+	struct drm_amdgpu_wait_cs_in in;
+	struct drm_amdgpu_wait_cs_out out;
+};
+
+struct drm_amdgpu_fence {
+	__u32 ctx_id;
+	__u32 ip_type;
+	__u32 ip_instance;
+	__u32 ring;
+	__u64 seq_no;
+};
+
+struct drm_amdgpu_wait_fences_in {
+	/** This points to uint64_t * which points to fences */
+	__u64 fences;
+	__u32 fence_count;
+	__u32 wait_all;
+	__u64 timeout_ns;
+};
+
+struct drm_amdgpu_wait_fences_out {
+	__u32 status;
+	__u32 first_signaled;
+};
+
+union drm_amdgpu_wait_fences {
+	struct drm_amdgpu_wait_fences_in in;
+	struct drm_amdgpu_wait_fences_out out;
+};
+
+#define AMDGPU_GEM_OP_GET_GEM_CREATE_INFO	0
+#define AMDGPU_GEM_OP_SET_PLACEMENT		1
+
+/* Sets or returns a value associated with a buffer. */
+struct drm_amdgpu_gem_op {
+	/** GEM object handle */
+	__u32	handle;
+	/** AMDGPU_GEM_OP_* */
+	__u32	op;
+	/** Input or return value */
+	__u64	value;
+};
+
+#define AMDGPU_VA_OP_MAP			1
+#define AMDGPU_VA_OP_UNMAP			2
+#define AMDGPU_VA_OP_CLEAR			3
+#define AMDGPU_VA_OP_REPLACE			4
+
+/* Delay the page table update till the next CS */
+#define AMDGPU_VM_DELAY_UPDATE		(1 << 0)
+
+/* Mapping flags */
+/* readable mapping */
+#define AMDGPU_VM_PAGE_READABLE		(1 << 1)
+/* writable mapping */
+#define AMDGPU_VM_PAGE_WRITEABLE	(1 << 2)
+/* executable mapping, new for VI */
+#define AMDGPU_VM_PAGE_EXECUTABLE	(1 << 3)
+/* partially resident texture */
+#define AMDGPU_VM_PAGE_PRT		(1 << 4)
+/* MTYPE flags use bit 5 to 8 */
+#define AMDGPU_VM_MTYPE_MASK		(0xf << 5)
+/* Default MTYPE. Pre-AI must use this.  Recommended for newer ASICs. */
+#define AMDGPU_VM_MTYPE_DEFAULT		(0 << 5)
+/* Use Non Coherent MTYPE instead of default MTYPE */
+#define AMDGPU_VM_MTYPE_NC		(1 << 5)
+/* Use Write Combine MTYPE instead of default MTYPE */
+#define AMDGPU_VM_MTYPE_WC		(2 << 5)
+/* Use Cache Coherent MTYPE instead of default MTYPE */
+#define AMDGPU_VM_MTYPE_CC		(3 << 5)
+/* Use UnCached MTYPE instead of default MTYPE */
+#define AMDGPU_VM_MTYPE_UC		(4 << 5)
+/* Use Read Write MTYPE instead of default MTYPE */
+#define AMDGPU_VM_MTYPE_RW		(5 << 5)
+/* don't allocate MALL */
+#define AMDGPU_VM_PAGE_NOALLOC		(1 << 9)
+
+struct drm_amdgpu_gem_va {
+	/** GEM object handle */
+	__u32 handle;
+	__u32 _pad;
+	/** AMDGPU_VA_OP_* */
+	__u32 operation;
+	/** AMDGPU_VM_PAGE_* */
+	__u32 flags;
+	/** va address to assign . Must be correctly aligned.*/
+	__u64 va_address;
+	/** Specify offset inside of BO to assign. Must be correctly aligned.*/
+	__u64 offset_in_bo;
+	/** Specify mapping size. Must be correctly aligned. */
+	__u64 map_size;
+};
+
+#define AMDGPU_HW_IP_GFX          0
+#define AMDGPU_HW_IP_COMPUTE      1
+#define AMDGPU_HW_IP_DMA          2
+#define AMDGPU_HW_IP_UVD          3
+#define AMDGPU_HW_IP_VCE          4
+#define AMDGPU_HW_IP_UVD_ENC      5
+#define AMDGPU_HW_IP_VCN_DEC      6
+/*
+ * From VCN4, AMDGPU_HW_IP_VCN_ENC is re-used to support
+ * both encoding and decoding jobs.
+ */
+#define AMDGPU_HW_IP_VCN_ENC      7
+#define AMDGPU_HW_IP_VCN_JPEG     8
+#define AMDGPU_HW_IP_NUM          9
+
+#define AMDGPU_HW_IP_INSTANCE_MAX_COUNT 1
+
+#define AMDGPU_CHUNK_ID_IB		0x01
+#define AMDGPU_CHUNK_ID_FENCE		0x02
+#define AMDGPU_CHUNK_ID_DEPENDENCIES	0x03
+#define AMDGPU_CHUNK_ID_SYNCOBJ_IN      0x04
+#define AMDGPU_CHUNK_ID_SYNCOBJ_OUT     0x05
+#define AMDGPU_CHUNK_ID_BO_HANDLES      0x06
+#define AMDGPU_CHUNK_ID_SCHEDULED_DEPENDENCIES	0x07
+#define AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_WAIT    0x08
+#define AMDGPU_CHUNK_ID_SYNCOBJ_TIMELINE_SIGNAL  0x09
+
+struct drm_amdgpu_cs_chunk {
+	__u32		chunk_id;
+	__u32		length_dw;
+	__u64		chunk_data;
+};
+
+struct drm_amdgpu_cs_in {
+	/** Rendering context id */
+	__u32		ctx_id;
+	/**  Handle of resource list associated with CS */
+	__u32		bo_list_handle;
+	__u32		num_chunks;
+	__u32		flags;
+	/** this points to __u64 * which point to cs chunks */
+	__u64		chunks;
+};
+
+struct drm_amdgpu_cs_out {
+	__u64 handle;
+};
+
+union drm_amdgpu_cs {
+	struct drm_amdgpu_cs_in in;
+	struct drm_amdgpu_cs_out out;
+};
+
+/* Specify flags to be used for IB */
+
+/* This IB should be submitted to CE */
+#define AMDGPU_IB_FLAG_CE	(1<<0)
+
+/* Preamble flag, which means the IB could be dropped if no context switch */
+#define AMDGPU_IB_FLAG_PREAMBLE (1<<1)
+
+/* Preempt flag, IB should set Pre_enb bit if PREEMPT flag detected */
+#define AMDGPU_IB_FLAG_PREEMPT (1<<2)
+
+/* The IB fence should do the L2 writeback but not invalidate any shader
+ * caches (L2/vL1/sL1/I$). */
+#define AMDGPU_IB_FLAG_TC_WB_NOT_INVALIDATE (1 << 3)
+
+/* Set GDS_COMPUTE_MAX_WAVE_ID = DEFAULT before PACKET3_INDIRECT_BUFFER.
+ * This will reset wave ID counters for the IB.
+ */
+#define AMDGPU_IB_FLAG_RESET_GDS_MAX_WAVE_ID (1 << 4)
+
+/* Flag the IB as secure (TMZ)
+ */
+#define AMDGPU_IB_FLAGS_SECURE  (1 << 5)
+
+/* Tell KMD to flush and invalidate caches
+ */
+#define AMDGPU_IB_FLAG_EMIT_MEM_SYNC  (1 << 6)
+
+struct drm_amdgpu_cs_chunk_ib {
+	__u32 _pad;
+	/** AMDGPU_IB_FLAG_* */
+	__u32 flags;
+	/** Virtual address to begin IB execution */
+	__u64 va_start;
+	/** Size of submission */
+	__u32 ib_bytes;
+	/** HW IP to submit to */
+	__u32 ip_type;
+	/** HW IP index of the same type to submit to  */
+	__u32 ip_instance;
+	/** Ring index to submit to */
+	__u32 ring;
+};
+
+struct drm_amdgpu_cs_chunk_dep {
+	__u32 ip_type;
+	__u32 ip_instance;
+	__u32 ring;
+	__u32 ctx_id;
+	__u64 handle;
+};
+
+struct drm_amdgpu_cs_chunk_fence {
+	__u32 handle;
+	__u32 offset;
+};
+
+struct drm_amdgpu_cs_chunk_sem {
+	__u32 handle;
+};
+
+struct drm_amdgpu_cs_chunk_syncobj {
+       __u32 handle;
+       __u32 flags;
+       __u64 point;
+};
+
+#define AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ	0
+#define AMDGPU_FENCE_TO_HANDLE_GET_SYNCOBJ_FD	1
+#define AMDGPU_FENCE_TO_HANDLE_GET_SYNC_FILE_FD	2
+
+union drm_amdgpu_fence_to_handle {
+	struct {
+		struct drm_amdgpu_fence fence;
+		__u32 what;
+		__u32 pad;
+	} in;
+	struct {
+		__u32 handle;
+	} out;
+};
+
+struct drm_amdgpu_cs_chunk_data {
+	union {
+		struct drm_amdgpu_cs_chunk_ib		ib_data;
+		struct drm_amdgpu_cs_chunk_fence	fence_data;
+	};
+};
+
+/*
+ *  Query h/w info: Flag that this is integrated (a.h.a. fusion) GPU
+ *
+ */
+#define AMDGPU_IDS_FLAGS_FUSION         0x1
+#define AMDGPU_IDS_FLAGS_PREEMPTION     0x2
+#define AMDGPU_IDS_FLAGS_TMZ            0x4
+
+/* indicate if acceleration can be working */
+#define AMDGPU_INFO_ACCEL_WORKING		0x00
+/* get the crtc_id from the mode object id? */
+#define AMDGPU_INFO_CRTC_FROM_ID		0x01
+/* query hw IP info */
+#define AMDGPU_INFO_HW_IP_INFO			0x02
+/* query hw IP instance count for the specified type */
+#define AMDGPU_INFO_HW_IP_COUNT			0x03
+/* timestamp for GL_ARB_timer_query */
+#define AMDGPU_INFO_TIMESTAMP			0x05
+/* Query the firmware version */
+#define AMDGPU_INFO_FW_VERSION			0x0e
+	/* Subquery id: Query VCE firmware version */
+	#define AMDGPU_INFO_FW_VCE		0x1
+	/* Subquery id: Query UVD firmware version */
+	#define AMDGPU_INFO_FW_UVD		0x2
+	/* Subquery id: Query GMC firmware version */
+	#define AMDGPU_INFO_FW_GMC		0x03
+	/* Subquery id: Query GFX ME firmware version */
+	#define AMDGPU_INFO_FW_GFX_ME		0x04
+	/* Subquery id: Query GFX PFP firmware version */
+	#define AMDGPU_INFO_FW_GFX_PFP		0x05
+	/* Subquery id: Query GFX CE firmware version */
+	#define AMDGPU_INFO_FW_GFX_CE		0x06
+	/* Subquery id: Query GFX RLC firmware version */
+	#define AMDGPU_INFO_FW_GFX_RLC		0x07
+	/* Subquery id: Query GFX MEC firmware version */
+	#define AMDGPU_INFO_FW_GFX_MEC		0x08
+	/* Subquery id: Query SMC firmware version */
+	#define AMDGPU_INFO_FW_SMC		0x0a
+	/* Subquery id: Query SDMA firmware version */
+	#define AMDGPU_INFO_FW_SDMA		0x0b
+	/* Subquery id: Query PSP SOS firmware version */
+	#define AMDGPU_INFO_FW_SOS		0x0c
+	/* Subquery id: Query PSP ASD firmware version */
+	#define AMDGPU_INFO_FW_ASD		0x0d
+	/* Subquery id: Query VCN firmware version */
+	#define AMDGPU_INFO_FW_VCN		0x0e
+	/* Subquery id: Query GFX RLC SRLC firmware version */
+	#define AMDGPU_INFO_FW_GFX_RLC_RESTORE_LIST_CNTL 0x0f
+	/* Subquery id: Query GFX RLC SRLG firmware version */
+	#define AMDGPU_INFO_FW_GFX_RLC_RESTORE_LIST_GPM_MEM 0x10
+	/* Subquery id: Query GFX RLC SRLS firmware version */
+	#define AMDGPU_INFO_FW_GFX_RLC_RESTORE_LIST_SRM_MEM 0x11
+	/* Subquery id: Query DMCU firmware version */
+	#define AMDGPU_INFO_FW_DMCU		0x12
+	#define AMDGPU_INFO_FW_TA		0x13
+	/* Subquery id: Query DMCUB firmware version */
+	#define AMDGPU_INFO_FW_DMCUB		0x14
+	/* Subquery id: Query TOC firmware version */
+	#define AMDGPU_INFO_FW_TOC		0x15
+	/* Subquery id: Query CAP firmware version */
+	#define AMDGPU_INFO_FW_CAP		0x16
+	/* Subquery id: Query GFX RLCP firmware version */
+	#define AMDGPU_INFO_FW_GFX_RLCP		0x17
+	/* Subquery id: Query GFX RLCV firmware version */
+	#define AMDGPU_INFO_FW_GFX_RLCV		0x18
+	/* Subquery id: Query MES_KIQ firmware version */
+	#define AMDGPU_INFO_FW_MES_KIQ		0x19
+	/* Subquery id: Query MES firmware version */
+	#define AMDGPU_INFO_FW_MES		0x1a
+	/* Subquery id: Query IMU firmware version */
+	#define AMDGPU_INFO_FW_IMU		0x1b
+
+/* number of bytes moved for TTM migration */
+#define AMDGPU_INFO_NUM_BYTES_MOVED		0x0f
+/* the used VRAM size */
+#define AMDGPU_INFO_VRAM_USAGE			0x10
+/* the used GTT size */
+#define AMDGPU_INFO_GTT_USAGE			0x11
+/* Information about GDS, etc. resource configuration */
+#define AMDGPU_INFO_GDS_CONFIG			0x13
+/* Query information about VRAM and GTT domains */
+#define AMDGPU_INFO_VRAM_GTT			0x14
+/* Query information about register in MMR address space*/
+#define AMDGPU_INFO_READ_MMR_REG		0x15
+/* Query information about device: rev id, family, etc. */
+#define AMDGPU_INFO_DEV_INFO			0x16
+/* visible vram usage */
+#define AMDGPU_INFO_VIS_VRAM_USAGE		0x17
+/* number of TTM buffer evictions */
+#define AMDGPU_INFO_NUM_EVICTIONS		0x18
+/* Query memory about VRAM and GTT domains */
+#define AMDGPU_INFO_MEMORY			0x19
+/* Query vce clock table */
+#define AMDGPU_INFO_VCE_CLOCK_TABLE		0x1A
+/* Query vbios related information */
+#define AMDGPU_INFO_VBIOS			0x1B
+	/* Subquery id: Query vbios size */
+	#define AMDGPU_INFO_VBIOS_SIZE		0x1
+	/* Subquery id: Query vbios image */
+	#define AMDGPU_INFO_VBIOS_IMAGE		0x2
+	/* Subquery id: Query vbios info */
+	#define AMDGPU_INFO_VBIOS_INFO		0x3
+/* Query UVD handles */
+#define AMDGPU_INFO_NUM_HANDLES			0x1C
+/* Query sensor related information */
+#define AMDGPU_INFO_SENSOR			0x1D
+	/* Subquery id: Query GPU shader clock */
+	#define AMDGPU_INFO_SENSOR_GFX_SCLK		0x1
+	/* Subquery id: Query GPU memory clock */
+	#define AMDGPU_INFO_SENSOR_GFX_MCLK		0x2
+	/* Subquery id: Query GPU temperature */
+	#define AMDGPU_INFO_SENSOR_GPU_TEMP		0x3
+	/* Subquery id: Query GPU load */
+	#define AMDGPU_INFO_SENSOR_GPU_LOAD		0x4
+	/* Subquery id: Query average GPU power	*/
+	#define AMDGPU_INFO_SENSOR_GPU_AVG_POWER	0x5
+	/* Subquery id: Query northbridge voltage */
+	#define AMDGPU_INFO_SENSOR_VDDNB		0x6
+	/* Subquery id: Query graphics voltage */
+	#define AMDGPU_INFO_SENSOR_VDDGFX		0x7
+	/* Subquery id: Query GPU stable pstate shader clock */
+	#define AMDGPU_INFO_SENSOR_STABLE_PSTATE_GFX_SCLK		0x8
+	/* Subquery id: Query GPU stable pstate memory clock */
+	#define AMDGPU_INFO_SENSOR_STABLE_PSTATE_GFX_MCLK		0x9
+/* Number of VRAM page faults on CPU access. */
+#define AMDGPU_INFO_NUM_VRAM_CPU_PAGE_FAULTS	0x1E
+#define AMDGPU_INFO_VRAM_LOST_COUNTER		0x1F
+/* query ras mask of enabled features*/
+#define AMDGPU_INFO_RAS_ENABLED_FEATURES	0x20
+/* RAS MASK: UMC (VRAM) */
+#define AMDGPU_INFO_RAS_ENABLED_UMC			(1 << 0)
+/* RAS MASK: SDMA */
+#define AMDGPU_INFO_RAS_ENABLED_SDMA			(1 << 1)
+/* RAS MASK: GFX */
+#define AMDGPU_INFO_RAS_ENABLED_GFX			(1 << 2)
+/* RAS MASK: MMHUB */
+#define AMDGPU_INFO_RAS_ENABLED_MMHUB			(1 << 3)
+/* RAS MASK: ATHUB */
+#define AMDGPU_INFO_RAS_ENABLED_ATHUB			(1 << 4)
+/* RAS MASK: PCIE */
+#define AMDGPU_INFO_RAS_ENABLED_PCIE			(1 << 5)
+/* RAS MASK: HDP */
+#define AMDGPU_INFO_RAS_ENABLED_HDP			(1 << 6)
+/* RAS MASK: XGMI */
+#define AMDGPU_INFO_RAS_ENABLED_XGMI			(1 << 7)
+/* RAS MASK: DF */
+#define AMDGPU_INFO_RAS_ENABLED_DF			(1 << 8)
+/* RAS MASK: SMN */
+#define AMDGPU_INFO_RAS_ENABLED_SMN			(1 << 9)
+/* RAS MASK: SEM */
+#define AMDGPU_INFO_RAS_ENABLED_SEM			(1 << 10)
+/* RAS MASK: MP0 */
+#define AMDGPU_INFO_RAS_ENABLED_MP0			(1 << 11)
+/* RAS MASK: MP1 */
+#define AMDGPU_INFO_RAS_ENABLED_MP1			(1 << 12)
+/* RAS MASK: FUSE */
+#define AMDGPU_INFO_RAS_ENABLED_FUSE			(1 << 13)
+/* query video encode/decode caps */
+#define AMDGPU_INFO_VIDEO_CAPS			0x21
+	/* Subquery id: Decode */
+	#define AMDGPU_INFO_VIDEO_CAPS_DECODE		0
+	/* Subquery id: Encode */
+	#define AMDGPU_INFO_VIDEO_CAPS_ENCODE		1
+
+#define AMDGPU_INFO_MMR_SE_INDEX_SHIFT	0
+#define AMDGPU_INFO_MMR_SE_INDEX_MASK	0xff
+#define AMDGPU_INFO_MMR_SH_INDEX_SHIFT	8
+#define AMDGPU_INFO_MMR_SH_INDEX_MASK	0xff
+
+struct drm_amdgpu_query_fw {
+	/** AMDGPU_INFO_FW_* */
+	__u32 fw_type;
+	/**
+	 * Index of the IP if there are more IPs of
+	 * the same type.
+	 */
+	__u32 ip_instance;
+	/**
+	 * Index of the engine. Whether this is used depends
+	 * on the firmware type. (e.g. MEC, SDMA)
+	 */
+	__u32 index;
+	__u32 _pad;
+};
+
+/* Input structure for the INFO ioctl */
+struct drm_amdgpu_info {
+	/* Where the return value will be stored */
+	__u64 return_pointer;
+	/* The size of the return value. Just like "size" in "snprintf",
+	 * it limits how many bytes the kernel can write. */
+	__u32 return_size;
+	/* The query request id. */
+	__u32 query;
+
+	union {
+		struct {
+			__u32 id;
+			__u32 _pad;
+		} mode_crtc;
+
+		struct {
+			/** AMDGPU_HW_IP_* */
+			__u32 type;
+			/**
+			 * Index of the IP if there are more IPs of the same
+			 * type. Ignored by AMDGPU_INFO_HW_IP_COUNT.
+			 */
+			__u32 ip_instance;
+		} query_hw_ip;
+
+		struct {
+			__u32 dword_offset;
+			/** number of registers to read */
+			__u32 count;
+			__u32 instance;
+			/** For future use, no flags defined so far */
+			__u32 flags;
+		} read_mmr_reg;
+
+		struct drm_amdgpu_query_fw query_fw;
+
+		struct {
+			__u32 type;
+			__u32 offset;
+		} vbios_info;
+
+		struct {
+			__u32 type;
+		} sensor_info;
+
+		struct {
+			__u32 type;
+		} video_cap;
+	};
+};
+
+struct drm_amdgpu_info_gds {
+	/** GDS GFX partition size */
+	__u32 gds_gfx_partition_size;
+	/** GDS compute partition size */
+	__u32 compute_partition_size;
+	/** total GDS memory size */
+	__u32 gds_total_size;
+	/** GWS size per GFX partition */
+	__u32 gws_per_gfx_partition;
+	/** GSW size per compute partition */
+	__u32 gws_per_compute_partition;
+	/** OA size per GFX partition */
+	__u32 oa_per_gfx_partition;
+	/** OA size per compute partition */
+	__u32 oa_per_compute_partition;
+	__u32 _pad;
+};
+
+struct drm_amdgpu_info_vram_gtt {
+	__u64 vram_size;
+	__u64 vram_cpu_accessible_size;
+	__u64 gtt_size;
+};
+
+struct drm_amdgpu_heap_info {
+	/** max. physical memory */
+	__u64 total_heap_size;
+
+	/** Theoretical max. available memory in the given heap */
+	__u64 usable_heap_size;
+
+	/**
+	 * Number of bytes allocated in the heap. This includes all processes
+	 * and private allocations in the kernel. It changes when new buffers
+	 * are allocated, freed, and moved. It cannot be larger than
+	 * heap_size.
+	 */
+	__u64 heap_usage;
+
+	/**
+	 * Theoretical possible max. size of buffer which
+	 * could be allocated in the given heap
+	 */
+	__u64 max_allocation;
+};
+
+struct drm_amdgpu_memory_info {
+	struct drm_amdgpu_heap_info vram;
+	struct drm_amdgpu_heap_info cpu_accessible_vram;
+	struct drm_amdgpu_heap_info gtt;
+};
+
+struct drm_amdgpu_info_firmware {
+	__u32 ver;
+	__u32 feature;
+};
+
+struct drm_amdgpu_info_vbios {
+	__u8 name[64];
+	__u8 vbios_pn[64];
+	__u32 version;
+	__u32 pad;
+	__u8 vbios_ver_str[32];
+	__u8 date[32];
+};
+
+#define AMDGPU_VRAM_TYPE_UNKNOWN 0
+#define AMDGPU_VRAM_TYPE_GDDR1 1
+#define AMDGPU_VRAM_TYPE_DDR2  2
+#define AMDGPU_VRAM_TYPE_GDDR3 3
+#define AMDGPU_VRAM_TYPE_GDDR4 4
+#define AMDGPU_VRAM_TYPE_GDDR5 5
+#define AMDGPU_VRAM_TYPE_HBM   6
+#define AMDGPU_VRAM_TYPE_DDR3  7
+#define AMDGPU_VRAM_TYPE_DDR4  8
+#define AMDGPU_VRAM_TYPE_GDDR6 9
+#define AMDGPU_VRAM_TYPE_DDR5  10
+#define AMDGPU_VRAM_TYPE_LPDDR4 11
+#define AMDGPU_VRAM_TYPE_LPDDR5 12
+
+struct drm_amdgpu_info_device {
+	/** PCI Device ID */
+	__u32 device_id;
+	/** Internal chip revision: A0, A1, etc.) */
+	__u32 chip_rev;
+	__u32 external_rev;
+	/** Revision id in PCI Config space */
+	__u32 pci_rev;
+	__u32 family;
+	__u32 num_shader_engines;
+	__u32 num_shader_arrays_per_engine;
+	/* in KHz */
+	__u32 gpu_counter_freq;
+	__u64 max_engine_clock;
+	__u64 max_memory_clock;
+	/* cu information */
+	__u32 cu_active_number;
+	/* NOTE: cu_ao_mask is INVALID, DON'T use it */
+	__u32 cu_ao_mask;
+	__u32 cu_bitmap[4][4];
+	/** Render backend pipe mask. One render backend is CB+DB. */
+	__u32 enabled_rb_pipes_mask;
+	__u32 num_rb_pipes;
+	__u32 num_hw_gfx_contexts;
+	__u32 _pad;
+	__u64 ids_flags;
+	/** Starting virtual address for UMDs. */
+	__u64 virtual_address_offset;
+	/** The maximum virtual address */
+	__u64 virtual_address_max;
+	/** Required alignment of virtual addresses. */
+	__u32 virtual_address_alignment;
+	/** Page table entry - fragment size */
+	__u32 pte_fragment_size;
+	__u32 gart_page_size;
+	/** constant engine ram size*/
+	__u32 ce_ram_size;
+	/** video memory type info*/
+	__u32 vram_type;
+	/** video memory bit width*/
+	__u32 vram_bit_width;
+	/* vce harvesting instance */
+	__u32 vce_harvest_config;
+	/* gfx double offchip LDS buffers */
+	__u32 gc_double_offchip_lds_buf;
+	/* NGG Primitive Buffer */
+	__u64 prim_buf_gpu_addr;
+	/* NGG Position Buffer */
+	__u64 pos_buf_gpu_addr;
+	/* NGG Control Sideband */
+	__u64 cntl_sb_buf_gpu_addr;
+	/* NGG Parameter Cache */
+	__u64 param_buf_gpu_addr;
+	__u32 prim_buf_size;
+	__u32 pos_buf_size;
+	__u32 cntl_sb_buf_size;
+	__u32 param_buf_size;
+	/* wavefront size*/
+	__u32 wave_front_size;
+	/* shader visible vgprs*/
+	__u32 num_shader_visible_vgprs;
+	/* CU per shader array*/
+	__u32 num_cu_per_sh;
+	/* number of tcc blocks*/
+	__u32 num_tcc_blocks;
+	/* gs vgt table depth*/
+	__u32 gs_vgt_table_depth;
+	/* gs primitive buffer depth*/
+	__u32 gs_prim_buffer_depth;
+	/* max gs wavefront per vgt*/
+	__u32 max_gs_waves_per_vgt;
+	__u32 _pad1;
+	/* always on cu bitmap */
+	__u32 cu_ao_bitmap[4][4];
+	/** Starting high virtual address for UMDs. */
+	__u64 high_va_offset;
+	/** The maximum high virtual address */
+	__u64 high_va_max;
+	/* gfx10 pa_sc_tile_steering_override */
+	__u32 pa_sc_tile_steering_override;
+	/* disabled TCCs */
+	__u64 tcc_disabled_mask;
+};
+
+struct drm_amdgpu_info_hw_ip {
+	/** Version of h/w IP */
+	__u32  hw_ip_version_major;
+	__u32  hw_ip_version_minor;
+	/** Capabilities */
+	__u64  capabilities_flags;
+	/** command buffer address start alignment*/
+	__u32  ib_start_alignment;
+	/** command buffer size alignment*/
+	__u32  ib_size_alignment;
+	/** Bitmask of available rings. Bit 0 means ring 0, etc. */
+	__u32  available_rings;
+	/** version info: bits 23:16 major, 15:8 minor, 7:0 revision */
+	__u32  ip_discovery_version;
+};
+
+struct drm_amdgpu_info_num_handles {
+	/** Max handles as supported by firmware for UVD */
+	__u32  uvd_max_handles;
+	/** Handles currently in use for UVD */
+	__u32  uvd_used_handles;
+};
+
+#define AMDGPU_VCE_CLOCK_TABLE_ENTRIES		6
+
+struct drm_amdgpu_info_vce_clock_table_entry {
+	/** System clock */
+	__u32 sclk;
+	/** Memory clock */
+	__u32 mclk;
+	/** VCE clock */
+	__u32 eclk;
+	__u32 pad;
+};
+
+struct drm_amdgpu_info_vce_clock_table {
+	struct drm_amdgpu_info_vce_clock_table_entry entries[AMDGPU_VCE_CLOCK_TABLE_ENTRIES];
+	__u32 num_valid_entries;
+	__u32 pad;
+};
+
+/* query video encode/decode caps */
+#define AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG2			0
+#define AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4			1
+#define AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_VC1			2
+#define AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC		3
+#define AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_HEVC			4
+#define AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_JPEG			5
+#define AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_VP9			6
+#define AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_AV1			7
+#define AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_COUNT			8
+
+struct drm_amdgpu_info_video_codec_info {
+	__u32 valid;
+	__u32 max_width;
+	__u32 max_height;
+	__u32 max_pixels_per_frame;
+	__u32 max_level;
+	__u32 pad;
+};
+
+struct drm_amdgpu_info_video_caps {
+	struct drm_amdgpu_info_video_codec_info codec_info[AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_COUNT];
+};
+
+/*
+ * Supported GPU families
+ */
+#define AMDGPU_FAMILY_UNKNOWN			0
+#define AMDGPU_FAMILY_SI			110 /* Hainan, Oland, Verde, Pitcairn, Tahiti */
+#define AMDGPU_FAMILY_CI			120 /* Bonaire, Hawaii */
+#define AMDGPU_FAMILY_KV			125 /* Kaveri, Kabini, Mullins */
+#define AMDGPU_FAMILY_VI			130 /* Iceland, Tonga */
+#define AMDGPU_FAMILY_CZ			135 /* Carrizo, Stoney */
+#define AMDGPU_FAMILY_AI			141 /* Vega10 */
+#define AMDGPU_FAMILY_RV			142 /* Raven */
+#define AMDGPU_FAMILY_NV			143 /* Navi10 */
+#define AMDGPU_FAMILY_VGH			144 /* Van Gogh */
+#define AMDGPU_FAMILY_GC_11_0_0			145 /* GC 11.0.0 */
+#define AMDGPU_FAMILY_YC			146 /* Yellow Carp */
+#define AMDGPU_FAMILY_GC_11_0_1			148 /* GC 11.0.1 */
+#define AMDGPU_FAMILY_GC_10_3_6			149 /* GC 10.3.6 */
+#define AMDGPU_FAMILY_GC_10_3_7			151 /* GC 10.3.7 */
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif
diff --git a/src/drm/drm-uapi/drm.h b/src/drm/drm-uapi/drm.h
new file mode 100644
index 00000000..45c28e5d
--- /dev/null
+++ b/src/drm/drm-uapi/drm.h
@@ -0,0 +1,1216 @@
+/*
+ * Header for the Direct Rendering Manager
+ *
+ * Author: Rickard E. (Rik) Faith <faith@valinux.com>
+ *
+ * Acknowledgments:
+ * Dec 1999, Richard Henderson <rth@twiddle.net>, move to generic cmpxchg.
+ */
+
+/*
+ * Copyright 1999 Precision Insight, Inc., Cedar Park, Texas.
+ * Copyright 2000 VA Linux Systems, Inc., Sunnyvale, California.
+ * All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * VA LINUX SYSTEMS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#ifndef _DRM_H_
+#define _DRM_H_
+
+#if   defined(__linux__)
+
+#include <linux/types.h>
+#include <asm/ioctl.h>
+typedef unsigned int drm_handle_t;
+
+#else /* One of the BSDs */
+
+#include <stdint.h>
+#include <sys/ioccom.h>
+#include <sys/types.h>
+typedef int8_t   __s8;
+typedef uint8_t  __u8;
+typedef int16_t  __s16;
+typedef uint16_t __u16;
+typedef int32_t  __s32;
+typedef uint32_t __u32;
+typedef int64_t  __s64;
+typedef uint64_t __u64;
+typedef size_t   __kernel_size_t;
+typedef unsigned long drm_handle_t;
+
+#endif
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+#define DRM_NAME	"drm"	  /**< Name in kernel, /dev, and /proc */
+#define DRM_MIN_ORDER	5	  /**< At least 2^5 bytes = 32 bytes */
+#define DRM_MAX_ORDER	22	  /**< Up to 2^22 bytes = 4MB */
+#define DRM_RAM_PERCENT 10	  /**< How much system ram can we lock? */
+
+#define _DRM_LOCK_HELD	0x80000000U /**< Hardware lock is held */
+#define _DRM_LOCK_CONT	0x40000000U /**< Hardware lock is contended */
+#define _DRM_LOCK_IS_HELD(lock)	   ((lock) & _DRM_LOCK_HELD)
+#define _DRM_LOCK_IS_CONT(lock)	   ((lock) & _DRM_LOCK_CONT)
+#define _DRM_LOCKING_CONTEXT(lock) ((lock) & ~(_DRM_LOCK_HELD|_DRM_LOCK_CONT))
+
+typedef unsigned int drm_context_t;
+typedef unsigned int drm_drawable_t;
+typedef unsigned int drm_magic_t;
+
+/*
+ * Cliprect.
+ *
+ * \warning: If you change this structure, make sure you change
+ * XF86DRIClipRectRec in the server as well
+ *
+ * \note KW: Actually it's illegal to change either for
+ * backwards-compatibility reasons.
+ */
+struct drm_clip_rect {
+	unsigned short x1;
+	unsigned short y1;
+	unsigned short x2;
+	unsigned short y2;
+};
+
+/*
+ * Drawable information.
+ */
+struct drm_drawable_info {
+	unsigned int num_rects;
+	struct drm_clip_rect *rects;
+};
+
+/*
+ * Texture region,
+ */
+struct drm_tex_region {
+	unsigned char next;
+	unsigned char prev;
+	unsigned char in_use;
+	unsigned char padding;
+	unsigned int age;
+};
+
+/*
+ * Hardware lock.
+ *
+ * The lock structure is a simple cache-line aligned integer.  To avoid
+ * processor bus contention on a multiprocessor system, there should not be any
+ * other data stored in the same cache line.
+ */
+struct drm_hw_lock {
+	__volatile__ unsigned int lock;		/**< lock variable */
+	char padding[60];			/**< Pad to cache line */
+};
+
+/*
+ * DRM_IOCTL_VERSION ioctl argument type.
+ *
+ * \sa drmGetVersion().
+ */
+struct drm_version {
+	int version_major;	  /**< Major version */
+	int version_minor;	  /**< Minor version */
+	int version_patchlevel;	  /**< Patch level */
+	__kernel_size_t name_len;	  /**< Length of name buffer */
+	char *name;	  /**< Name of driver */
+	__kernel_size_t date_len;	  /**< Length of date buffer */
+	char *date;	  /**< User-space buffer to hold date */
+	__kernel_size_t desc_len;	  /**< Length of desc buffer */
+	char *desc;	  /**< User-space buffer to hold desc */
+};
+
+/*
+ * DRM_IOCTL_GET_UNIQUE ioctl argument type.
+ *
+ * \sa drmGetBusid() and drmSetBusId().
+ */
+struct drm_unique {
+	__kernel_size_t unique_len;	  /**< Length of unique */
+	char *unique;	  /**< Unique name for driver instantiation */
+};
+
+struct drm_list {
+	int count;		  /**< Length of user-space structures */
+	struct drm_version *version;
+};
+
+struct drm_block {
+	int unused;
+};
+
+/*
+ * DRM_IOCTL_CONTROL ioctl argument type.
+ *
+ * \sa drmCtlInstHandler() and drmCtlUninstHandler().
+ */
+struct drm_control {
+	enum {
+		DRM_ADD_COMMAND,
+		DRM_RM_COMMAND,
+		DRM_INST_HANDLER,
+		DRM_UNINST_HANDLER
+	} func;
+	int irq;
+};
+
+/*
+ * Type of memory to map.
+ */
+enum drm_map_type {
+	_DRM_FRAME_BUFFER = 0,	  /**< WC (no caching), no core dump */
+	_DRM_REGISTERS = 1,	  /**< no caching, no core dump */
+	_DRM_SHM = 2,		  /**< shared, cached */
+	_DRM_AGP = 3,		  /**< AGP/GART */
+	_DRM_SCATTER_GATHER = 4,  /**< Scatter/gather memory for PCI DMA */
+	_DRM_CONSISTENT = 5	  /**< Consistent memory for PCI DMA */
+};
+
+/*
+ * Memory mapping flags.
+ */
+enum drm_map_flags {
+	_DRM_RESTRICTED = 0x01,	     /**< Cannot be mapped to user-virtual */
+	_DRM_READ_ONLY = 0x02,
+	_DRM_LOCKED = 0x04,	     /**< shared, cached, locked */
+	_DRM_KERNEL = 0x08,	     /**< kernel requires access */
+	_DRM_WRITE_COMBINING = 0x10, /**< use write-combining if available */
+	_DRM_CONTAINS_LOCK = 0x20,   /**< SHM page that contains lock */
+	_DRM_REMOVABLE = 0x40,	     /**< Removable mapping */
+	_DRM_DRIVER = 0x80	     /**< Managed by driver */
+};
+
+struct drm_ctx_priv_map {
+	unsigned int ctx_id;	 /**< Context requesting private mapping */
+	void *handle;		 /**< Handle of map */
+};
+
+/*
+ * DRM_IOCTL_GET_MAP, DRM_IOCTL_ADD_MAP and DRM_IOCTL_RM_MAP ioctls
+ * argument type.
+ *
+ * \sa drmAddMap().
+ */
+struct drm_map {
+	unsigned long offset;	 /**< Requested physical address (0 for SAREA)*/
+	unsigned long size;	 /**< Requested physical size (bytes) */
+	enum drm_map_type type;	 /**< Type of memory to map */
+	enum drm_map_flags flags;	 /**< Flags */
+	void *handle;		 /**< User-space: "Handle" to pass to mmap() */
+				 /**< Kernel-space: kernel-virtual address */
+	int mtrr;		 /**< MTRR slot used */
+	/*   Private data */
+};
+
+struct drm_set_name {
+	char name[32];
+};
+
+/*
+ * DRM_IOCTL_GET_CLIENT ioctl argument type.
+ */
+struct drm_client {
+	int idx;		/**< Which client desired? */
+	int auth;		/**< Is client authenticated? */
+	unsigned long pid;	/**< Process ID */
+	unsigned long uid;	/**< User ID */
+	unsigned long magic;	/**< Magic */
+	unsigned long iocs;	/**< Ioctl count */
+};
+
+enum drm_stat_type {
+	_DRM_STAT_LOCK,
+	_DRM_STAT_OPENS,
+	_DRM_STAT_CLOSES,
+	_DRM_STAT_IOCTLS,
+	_DRM_STAT_LOCKS,
+	_DRM_STAT_UNLOCKS,
+	_DRM_STAT_VALUE,	/**< Generic value */
+	_DRM_STAT_BYTE,		/**< Generic byte counter (1024bytes/K) */
+	_DRM_STAT_COUNT,	/**< Generic non-byte counter (1000/k) */
+
+	_DRM_STAT_IRQ,		/**< IRQ */
+	_DRM_STAT_PRIMARY,	/**< Primary DMA bytes */
+	_DRM_STAT_SECONDARY,	/**< Secondary DMA bytes */
+	_DRM_STAT_DMA,		/**< DMA */
+	_DRM_STAT_SPECIAL,	/**< Special DMA (e.g., priority or polled) */
+	_DRM_STAT_MISSED	/**< Missed DMA opportunity */
+	    /* Add to the *END* of the list */
+};
+
+/*
+ * DRM_IOCTL_GET_STATS ioctl argument type.
+ */
+struct drm_stats {
+	unsigned long count;
+	struct {
+		unsigned long value;
+		enum drm_stat_type type;
+	} data[15];
+};
+
+/*
+ * Hardware locking flags.
+ */
+enum drm_lock_flags {
+	_DRM_LOCK_READY = 0x01,	     /**< Wait until hardware is ready for DMA */
+	_DRM_LOCK_QUIESCENT = 0x02,  /**< Wait until hardware quiescent */
+	_DRM_LOCK_FLUSH = 0x04,	     /**< Flush this context's DMA queue first */
+	_DRM_LOCK_FLUSH_ALL = 0x08,  /**< Flush all DMA queues first */
+	/* These *HALT* flags aren't supported yet
+	   -- they will be used to support the
+	   full-screen DGA-like mode. */
+	_DRM_HALT_ALL_QUEUES = 0x10, /**< Halt all current and future queues */
+	_DRM_HALT_CUR_QUEUES = 0x20  /**< Halt all current queues */
+};
+
+/*
+ * DRM_IOCTL_LOCK, DRM_IOCTL_UNLOCK and DRM_IOCTL_FINISH ioctl argument type.
+ *
+ * \sa drmGetLock() and drmUnlock().
+ */
+struct drm_lock {
+	int context;
+	enum drm_lock_flags flags;
+};
+
+/*
+ * DMA flags
+ *
+ * \warning
+ * These values \e must match xf86drm.h.
+ *
+ * \sa drm_dma.
+ */
+enum drm_dma_flags {
+	/* Flags for DMA buffer dispatch */
+	_DRM_DMA_BLOCK = 0x01,	      /**<
+				       * Block until buffer dispatched.
+				       *
+				       * \note The buffer may not yet have
+				       * been processed by the hardware --
+				       * getting a hardware lock with the
+				       * hardware quiescent will ensure
+				       * that the buffer has been
+				       * processed.
+				       */
+	_DRM_DMA_WHILE_LOCKED = 0x02, /**< Dispatch while lock held */
+	_DRM_DMA_PRIORITY = 0x04,     /**< High priority dispatch */
+
+	/* Flags for DMA buffer request */
+	_DRM_DMA_WAIT = 0x10,	      /**< Wait for free buffers */
+	_DRM_DMA_SMALLER_OK = 0x20,   /**< Smaller-than-requested buffers OK */
+	_DRM_DMA_LARGER_OK = 0x40     /**< Larger-than-requested buffers OK */
+};
+
+/*
+ * DRM_IOCTL_ADD_BUFS and DRM_IOCTL_MARK_BUFS ioctl argument type.
+ *
+ * \sa drmAddBufs().
+ */
+struct drm_buf_desc {
+	int count;		 /**< Number of buffers of this size */
+	int size;		 /**< Size in bytes */
+	int low_mark;		 /**< Low water mark */
+	int high_mark;		 /**< High water mark */
+	enum {
+		_DRM_PAGE_ALIGN = 0x01,	/**< Align on page boundaries for DMA */
+		_DRM_AGP_BUFFER = 0x02,	/**< Buffer is in AGP space */
+		_DRM_SG_BUFFER = 0x04,	/**< Scatter/gather memory buffer */
+		_DRM_FB_BUFFER = 0x08,	/**< Buffer is in frame buffer */
+		_DRM_PCI_BUFFER_RO = 0x10 /**< Map PCI DMA buffer read-only */
+	} flags;
+	unsigned long agp_start; /**<
+				  * Start address of where the AGP buffers are
+				  * in the AGP aperture
+				  */
+};
+
+/*
+ * DRM_IOCTL_INFO_BUFS ioctl argument type.
+ */
+struct drm_buf_info {
+	int count;		/**< Entries in list */
+	struct drm_buf_desc *list;
+};
+
+/*
+ * DRM_IOCTL_FREE_BUFS ioctl argument type.
+ */
+struct drm_buf_free {
+	int count;
+	int *list;
+};
+
+/*
+ * Buffer information
+ *
+ * \sa drm_buf_map.
+ */
+struct drm_buf_pub {
+	int idx;		       /**< Index into the master buffer list */
+	int total;		       /**< Buffer size */
+	int used;		       /**< Amount of buffer in use (for DMA) */
+	void *address;	       /**< Address of buffer */
+};
+
+/*
+ * DRM_IOCTL_MAP_BUFS ioctl argument type.
+ */
+struct drm_buf_map {
+	int count;		/**< Length of the buffer list */
+#ifdef __cplusplus
+	void *virt;
+#else
+	void *virtual;		/**< Mmap'd area in user-virtual */
+#endif
+	struct drm_buf_pub *list;	/**< Buffer information */
+};
+
+/*
+ * DRM_IOCTL_DMA ioctl argument type.
+ *
+ * Indices here refer to the offset into the buffer list in drm_buf_get.
+ *
+ * \sa drmDMA().
+ */
+struct drm_dma {
+	int context;			  /**< Context handle */
+	int send_count;			  /**< Number of buffers to send */
+	int *send_indices;	  /**< List of handles to buffers */
+	int *send_sizes;		  /**< Lengths of data to send */
+	enum drm_dma_flags flags;	  /**< Flags */
+	int request_count;		  /**< Number of buffers requested */
+	int request_size;		  /**< Desired size for buffers */
+	int *request_indices;	  /**< Buffer information */
+	int *request_sizes;
+	int granted_count;		  /**< Number of buffers granted */
+};
+
+enum drm_ctx_flags {
+	_DRM_CONTEXT_PRESERVED = 0x01,
+	_DRM_CONTEXT_2DONLY = 0x02
+};
+
+/*
+ * DRM_IOCTL_ADD_CTX ioctl argument type.
+ *
+ * \sa drmCreateContext() and drmDestroyContext().
+ */
+struct drm_ctx {
+	drm_context_t handle;
+	enum drm_ctx_flags flags;
+};
+
+/*
+ * DRM_IOCTL_RES_CTX ioctl argument type.
+ */
+struct drm_ctx_res {
+	int count;
+	struct drm_ctx *contexts;
+};
+
+/*
+ * DRM_IOCTL_ADD_DRAW and DRM_IOCTL_RM_DRAW ioctl argument type.
+ */
+struct drm_draw {
+	drm_drawable_t handle;
+};
+
+/*
+ * DRM_IOCTL_UPDATE_DRAW ioctl argument type.
+ */
+typedef enum {
+	DRM_DRAWABLE_CLIPRECTS
+} drm_drawable_info_type_t;
+
+struct drm_update_draw {
+	drm_drawable_t handle;
+	unsigned int type;
+	unsigned int num;
+	unsigned long long data;
+};
+
+/*
+ * DRM_IOCTL_GET_MAGIC and DRM_IOCTL_AUTH_MAGIC ioctl argument type.
+ */
+struct drm_auth {
+	drm_magic_t magic;
+};
+
+/*
+ * DRM_IOCTL_IRQ_BUSID ioctl argument type.
+ *
+ * \sa drmGetInterruptFromBusID().
+ */
+struct drm_irq_busid {
+	int irq;	/**< IRQ number */
+	int busnum;	/**< bus number */
+	int devnum;	/**< device number */
+	int funcnum;	/**< function number */
+};
+
+enum drm_vblank_seq_type {
+	_DRM_VBLANK_ABSOLUTE = 0x0,	/**< Wait for specific vblank sequence number */
+	_DRM_VBLANK_RELATIVE = 0x1,	/**< Wait for given number of vblanks */
+	/* bits 1-6 are reserved for high crtcs */
+	_DRM_VBLANK_HIGH_CRTC_MASK = 0x0000003e,
+	_DRM_VBLANK_EVENT = 0x4000000,   /**< Send event instead of blocking */
+	_DRM_VBLANK_FLIP = 0x8000000,   /**< Scheduled buffer swap should flip */
+	_DRM_VBLANK_NEXTONMISS = 0x10000000,	/**< If missed, wait for next vblank */
+	_DRM_VBLANK_SECONDARY = 0x20000000,	/**< Secondary display controller */
+	_DRM_VBLANK_SIGNAL = 0x40000000	/**< Send signal instead of blocking, unsupported */
+};
+#define _DRM_VBLANK_HIGH_CRTC_SHIFT 1
+
+#define _DRM_VBLANK_TYPES_MASK (_DRM_VBLANK_ABSOLUTE | _DRM_VBLANK_RELATIVE)
+#define _DRM_VBLANK_FLAGS_MASK (_DRM_VBLANK_EVENT | _DRM_VBLANK_SIGNAL | \
+				_DRM_VBLANK_SECONDARY | _DRM_VBLANK_NEXTONMISS)
+
+struct drm_wait_vblank_request {
+	enum drm_vblank_seq_type type;
+	unsigned int sequence;
+	unsigned long signal;
+};
+
+struct drm_wait_vblank_reply {
+	enum drm_vblank_seq_type type;
+	unsigned int sequence;
+	long tval_sec;
+	long tval_usec;
+};
+
+/*
+ * DRM_IOCTL_WAIT_VBLANK ioctl argument type.
+ *
+ * \sa drmWaitVBlank().
+ */
+union drm_wait_vblank {
+	struct drm_wait_vblank_request request;
+	struct drm_wait_vblank_reply reply;
+};
+
+#define _DRM_PRE_MODESET 1
+#define _DRM_POST_MODESET 2
+
+/*
+ * DRM_IOCTL_MODESET_CTL ioctl argument type
+ *
+ * \sa drmModesetCtl().
+ */
+struct drm_modeset_ctl {
+	__u32 crtc;
+	__u32 cmd;
+};
+
+/*
+ * DRM_IOCTL_AGP_ENABLE ioctl argument type.
+ *
+ * \sa drmAgpEnable().
+ */
+struct drm_agp_mode {
+	unsigned long mode;	/**< AGP mode */
+};
+
+/*
+ * DRM_IOCTL_AGP_ALLOC and DRM_IOCTL_AGP_FREE ioctls argument type.
+ *
+ * \sa drmAgpAlloc() and drmAgpFree().
+ */
+struct drm_agp_buffer {
+	unsigned long size;	/**< In bytes -- will round to page boundary */
+	unsigned long handle;	/**< Used for binding / unbinding */
+	unsigned long type;	/**< Type of memory to allocate */
+	unsigned long physical;	/**< Physical used by i810 */
+};
+
+/*
+ * DRM_IOCTL_AGP_BIND and DRM_IOCTL_AGP_UNBIND ioctls argument type.
+ *
+ * \sa drmAgpBind() and drmAgpUnbind().
+ */
+struct drm_agp_binding {
+	unsigned long handle;	/**< From drm_agp_buffer */
+	unsigned long offset;	/**< In bytes -- will round to page boundary */
+};
+
+/*
+ * DRM_IOCTL_AGP_INFO ioctl argument type.
+ *
+ * \sa drmAgpVersionMajor(), drmAgpVersionMinor(), drmAgpGetMode(),
+ * drmAgpBase(), drmAgpSize(), drmAgpMemoryUsed(), drmAgpMemoryAvail(),
+ * drmAgpVendorId() and drmAgpDeviceId().
+ */
+struct drm_agp_info {
+	int agp_version_major;
+	int agp_version_minor;
+	unsigned long mode;
+	unsigned long aperture_base;	/* physical address */
+	unsigned long aperture_size;	/* bytes */
+	unsigned long memory_allowed;	/* bytes */
+	unsigned long memory_used;
+
+	/* PCI information */
+	unsigned short id_vendor;
+	unsigned short id_device;
+};
+
+/*
+ * DRM_IOCTL_SG_ALLOC ioctl argument type.
+ */
+struct drm_scatter_gather {
+	unsigned long size;	/**< In bytes -- will round to page boundary */
+	unsigned long handle;	/**< Used for mapping / unmapping */
+};
+
+/*
+ * DRM_IOCTL_SET_VERSION ioctl argument type.
+ */
+struct drm_set_version {
+	int drm_di_major;
+	int drm_di_minor;
+	int drm_dd_major;
+	int drm_dd_minor;
+};
+
+/* DRM_IOCTL_GEM_CLOSE ioctl argument type */
+struct drm_gem_close {
+	/** Handle of the object to be closed. */
+	__u32 handle;
+	__u32 pad;
+};
+
+/* DRM_IOCTL_GEM_FLINK ioctl argument type */
+struct drm_gem_flink {
+	/** Handle for the object being named */
+	__u32 handle;
+
+	/** Returned global name */
+	__u32 name;
+};
+
+/* DRM_IOCTL_GEM_OPEN ioctl argument type */
+struct drm_gem_open {
+	/** Name of object being opened */
+	__u32 name;
+
+	/** Returned handle for the object */
+	__u32 handle;
+
+	/** Returned size of the object */
+	__u64 size;
+};
+
+/**
+ * DRM_CAP_DUMB_BUFFER
+ *
+ * If set to 1, the driver supports creating dumb buffers via the
+ * &DRM_IOCTL_MODE_CREATE_DUMB ioctl.
+ */
+#define DRM_CAP_DUMB_BUFFER		0x1
+/**
+ * DRM_CAP_VBLANK_HIGH_CRTC
+ *
+ * If set to 1, the kernel supports specifying a :ref:`CRTC index<crtc_index>`
+ * in the high bits of &drm_wait_vblank_request.type.
+ *
+ * Starting kernel version 2.6.39, this capability is always set to 1.
+ */
+#define DRM_CAP_VBLANK_HIGH_CRTC	0x2
+/**
+ * DRM_CAP_DUMB_PREFERRED_DEPTH
+ *
+ * The preferred bit depth for dumb buffers.
+ *
+ * The bit depth is the number of bits used to indicate the color of a single
+ * pixel excluding any padding. This is different from the number of bits per
+ * pixel. For instance, XRGB8888 has a bit depth of 24 but has 32 bits per
+ * pixel.
+ *
+ * Note that this preference only applies to dumb buffers, it's irrelevant for
+ * other types of buffers.
+ */
+#define DRM_CAP_DUMB_PREFERRED_DEPTH	0x3
+/**
+ * DRM_CAP_DUMB_PREFER_SHADOW
+ *
+ * If set to 1, the driver prefers userspace to render to a shadow buffer
+ * instead of directly rendering to a dumb buffer. For best speed, userspace
+ * should do streaming ordered memory copies into the dumb buffer and never
+ * read from it.
+ *
+ * Note that this preference only applies to dumb buffers, it's irrelevant for
+ * other types of buffers.
+ */
+#define DRM_CAP_DUMB_PREFER_SHADOW	0x4
+/**
+ * DRM_CAP_PRIME
+ *
+ * Bitfield of supported PRIME sharing capabilities. See &DRM_PRIME_CAP_IMPORT
+ * and &DRM_PRIME_CAP_EXPORT.
+ *
+ * PRIME buffers are exposed as dma-buf file descriptors. See
+ * Documentation/gpu/drm-mm.rst, section "PRIME Buffer Sharing".
+ */
+#define DRM_CAP_PRIME			0x5
+/**
+ * DRM_PRIME_CAP_IMPORT
+ *
+ * If this bit is set in &DRM_CAP_PRIME, the driver supports importing PRIME
+ * buffers via the &DRM_IOCTL_PRIME_FD_TO_HANDLE ioctl.
+ */
+#define  DRM_PRIME_CAP_IMPORT		0x1
+/**
+ * DRM_PRIME_CAP_EXPORT
+ *
+ * If this bit is set in &DRM_CAP_PRIME, the driver supports exporting PRIME
+ * buffers via the &DRM_IOCTL_PRIME_HANDLE_TO_FD ioctl.
+ */
+#define  DRM_PRIME_CAP_EXPORT		0x2
+/**
+ * DRM_CAP_TIMESTAMP_MONOTONIC
+ *
+ * If set to 0, the kernel will report timestamps with ``CLOCK_REALTIME`` in
+ * struct drm_event_vblank. If set to 1, the kernel will report timestamps with
+ * ``CLOCK_MONOTONIC``. See ``clock_gettime(2)`` for the definition of these
+ * clocks.
+ *
+ * Starting from kernel version 2.6.39, the default value for this capability
+ * is 1. Starting kernel version 4.15, this capability is always set to 1.
+ */
+#define DRM_CAP_TIMESTAMP_MONOTONIC	0x6
+/**
+ * DRM_CAP_ASYNC_PAGE_FLIP
+ *
+ * If set to 1, the driver supports &DRM_MODE_PAGE_FLIP_ASYNC.
+ */
+#define DRM_CAP_ASYNC_PAGE_FLIP		0x7
+/**
+ * DRM_CAP_CURSOR_WIDTH
+ *
+ * The ``CURSOR_WIDTH`` and ``CURSOR_HEIGHT`` capabilities return a valid
+ * width x height combination for the hardware cursor. The intention is that a
+ * hardware agnostic userspace can query a cursor plane size to use.
+ *
+ * Note that the cross-driver contract is to merely return a valid size;
+ * drivers are free to attach another meaning on top, eg. i915 returns the
+ * maximum plane size.
+ */
+#define DRM_CAP_CURSOR_WIDTH		0x8
+/**
+ * DRM_CAP_CURSOR_HEIGHT
+ *
+ * See &DRM_CAP_CURSOR_WIDTH.
+ */
+#define DRM_CAP_CURSOR_HEIGHT		0x9
+/**
+ * DRM_CAP_ADDFB2_MODIFIERS
+ *
+ * If set to 1, the driver supports supplying modifiers in the
+ * &DRM_IOCTL_MODE_ADDFB2 ioctl.
+ */
+#define DRM_CAP_ADDFB2_MODIFIERS	0x10
+/**
+ * DRM_CAP_PAGE_FLIP_TARGET
+ *
+ * If set to 1, the driver supports the &DRM_MODE_PAGE_FLIP_TARGET_ABSOLUTE and
+ * &DRM_MODE_PAGE_FLIP_TARGET_RELATIVE flags in
+ * &drm_mode_crtc_page_flip_target.flags for the &DRM_IOCTL_MODE_PAGE_FLIP
+ * ioctl.
+ */
+#define DRM_CAP_PAGE_FLIP_TARGET	0x11
+/**
+ * DRM_CAP_CRTC_IN_VBLANK_EVENT
+ *
+ * If set to 1, the kernel supports reporting the CRTC ID in
+ * &drm_event_vblank.crtc_id for the &DRM_EVENT_VBLANK and
+ * &DRM_EVENT_FLIP_COMPLETE events.
+ *
+ * Starting kernel version 4.12, this capability is always set to 1.
+ */
+#define DRM_CAP_CRTC_IN_VBLANK_EVENT	0x12
+/**
+ * DRM_CAP_SYNCOBJ
+ *
+ * If set to 1, the driver supports sync objects. See
+ * Documentation/gpu/drm-mm.rst, section "DRM Sync Objects".
+ */
+#define DRM_CAP_SYNCOBJ		0x13
+/**
+ * DRM_CAP_SYNCOBJ_TIMELINE
+ *
+ * If set to 1, the driver supports timeline operations on sync objects. See
+ * Documentation/gpu/drm-mm.rst, section "DRM Sync Objects".
+ */
+#define DRM_CAP_SYNCOBJ_TIMELINE	0x14
+
+/* DRM_IOCTL_GET_CAP ioctl argument type */
+struct drm_get_cap {
+	__u64 capability;
+	__u64 value;
+};
+
+/**
+ * DRM_CLIENT_CAP_STEREO_3D
+ *
+ * If set to 1, the DRM core will expose the stereo 3D capabilities of the
+ * monitor by advertising the supported 3D layouts in the flags of struct
+ * drm_mode_modeinfo. See ``DRM_MODE_FLAG_3D_*``.
+ *
+ * This capability is always supported for all drivers starting from kernel
+ * version 3.13.
+ */
+#define DRM_CLIENT_CAP_STEREO_3D	1
+
+/**
+ * DRM_CLIENT_CAP_UNIVERSAL_PLANES
+ *
+ * If set to 1, the DRM core will expose all planes (overlay, primary, and
+ * cursor) to userspace.
+ *
+ * This capability has been introduced in kernel version 3.15. Starting from
+ * kernel version 3.17, this capability is always supported for all drivers.
+ */
+#define DRM_CLIENT_CAP_UNIVERSAL_PLANES  2
+
+/**
+ * DRM_CLIENT_CAP_ATOMIC
+ *
+ * If set to 1, the DRM core will expose atomic properties to userspace. This
+ * implicitly enables &DRM_CLIENT_CAP_UNIVERSAL_PLANES and
+ * &DRM_CLIENT_CAP_ASPECT_RATIO.
+ *
+ * If the driver doesn't support atomic mode-setting, enabling this capability
+ * will fail with -EOPNOTSUPP.
+ *
+ * This capability has been introduced in kernel version 4.0. Starting from
+ * kernel version 4.2, this capability is always supported for atomic-capable
+ * drivers.
+ */
+#define DRM_CLIENT_CAP_ATOMIC	3
+
+/**
+ * DRM_CLIENT_CAP_ASPECT_RATIO
+ *
+ * If set to 1, the DRM core will provide aspect ratio information in modes.
+ * See ``DRM_MODE_FLAG_PIC_AR_*``.
+ *
+ * This capability is always supported for all drivers starting from kernel
+ * version 4.18.
+ */
+#define DRM_CLIENT_CAP_ASPECT_RATIO    4
+
+/**
+ * DRM_CLIENT_CAP_WRITEBACK_CONNECTORS
+ *
+ * If set to 1, the DRM core will expose special connectors to be used for
+ * writing back to memory the scene setup in the commit. The client must enable
+ * &DRM_CLIENT_CAP_ATOMIC first.
+ *
+ * This capability is always supported for atomic-capable drivers starting from
+ * kernel version 4.19.
+ */
+#define DRM_CLIENT_CAP_WRITEBACK_CONNECTORS	5
+
+/* DRM_IOCTL_SET_CLIENT_CAP ioctl argument type */
+struct drm_set_client_cap {
+	__u64 capability;
+	__u64 value;
+};
+
+#define DRM_RDWR O_RDWR
+#define DRM_CLOEXEC O_CLOEXEC
+struct drm_prime_handle {
+	__u32 handle;
+
+	/** Flags.. only applicable for handle->fd */
+	__u32 flags;
+
+	/** Returned dmabuf file descriptor */
+	__s32 fd;
+};
+
+struct drm_syncobj_create {
+	__u32 handle;
+#define DRM_SYNCOBJ_CREATE_SIGNALED (1 << 0)
+	__u32 flags;
+};
+
+struct drm_syncobj_destroy {
+	__u32 handle;
+	__u32 pad;
+};
+
+#define DRM_SYNCOBJ_FD_TO_HANDLE_FLAGS_IMPORT_SYNC_FILE (1 << 0)
+#define DRM_SYNCOBJ_HANDLE_TO_FD_FLAGS_EXPORT_SYNC_FILE (1 << 0)
+struct drm_syncobj_handle {
+	__u32 handle;
+	__u32 flags;
+
+	__s32 fd;
+	__u32 pad;
+};
+
+struct drm_syncobj_transfer {
+	__u32 src_handle;
+	__u32 dst_handle;
+	__u64 src_point;
+	__u64 dst_point;
+	__u32 flags;
+	__u32 pad;
+};
+
+#define DRM_SYNCOBJ_WAIT_FLAGS_WAIT_ALL (1 << 0)
+#define DRM_SYNCOBJ_WAIT_FLAGS_WAIT_FOR_SUBMIT (1 << 1)
+#define DRM_SYNCOBJ_WAIT_FLAGS_WAIT_AVAILABLE (1 << 2) /* wait for time point to become available */
+struct drm_syncobj_wait {
+	__u64 handles;
+	/* absolute timeout */
+	__s64 timeout_nsec;
+	__u32 count_handles;
+	__u32 flags;
+	__u32 first_signaled; /* only valid when not waiting all */
+	__u32 pad;
+};
+
+struct drm_syncobj_timeline_wait {
+	__u64 handles;
+	/* wait on specific timeline point for every handles*/
+	__u64 points;
+	/* absolute timeout */
+	__s64 timeout_nsec;
+	__u32 count_handles;
+	__u32 flags;
+	__u32 first_signaled; /* only valid when not waiting all */
+	__u32 pad;
+};
+
+
+struct drm_syncobj_array {
+	__u64 handles;
+	__u32 count_handles;
+	__u32 pad;
+};
+
+#define DRM_SYNCOBJ_QUERY_FLAGS_LAST_SUBMITTED (1 << 0) /* last available point on timeline syncobj */
+struct drm_syncobj_timeline_array {
+	__u64 handles;
+	__u64 points;
+	__u32 count_handles;
+	__u32 flags;
+};
+
+
+/* Query current scanout sequence number */
+struct drm_crtc_get_sequence {
+	__u32 crtc_id;		/* requested crtc_id */
+	__u32 active;		/* return: crtc output is active */
+	__u64 sequence;		/* return: most recent vblank sequence */
+	__s64 sequence_ns;	/* return: most recent time of first pixel out */
+};
+
+/* Queue event to be delivered at specified sequence. Time stamp marks
+ * when the first pixel of the refresh cycle leaves the display engine
+ * for the display
+ */
+#define DRM_CRTC_SEQUENCE_RELATIVE		0x00000001	/* sequence is relative to current */
+#define DRM_CRTC_SEQUENCE_NEXT_ON_MISS		0x00000002	/* Use next sequence if we've missed */
+
+struct drm_crtc_queue_sequence {
+	__u32 crtc_id;
+	__u32 flags;
+	__u64 sequence;		/* on input, target sequence. on output, actual sequence */
+	__u64 user_data;	/* user data passed to event */
+};
+
+#if defined(__cplusplus)
+}
+#endif
+
+#include "drm_mode.h"
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+#define DRM_IOCTL_BASE			'd'
+#define DRM_IO(nr)			_IO(DRM_IOCTL_BASE,nr)
+#define DRM_IOR(nr,type)		_IOR(DRM_IOCTL_BASE,nr,type)
+#define DRM_IOW(nr,type)		_IOW(DRM_IOCTL_BASE,nr,type)
+#define DRM_IOWR(nr,type)		_IOWR(DRM_IOCTL_BASE,nr,type)
+
+#define DRM_IOCTL_VERSION		DRM_IOWR(0x00, struct drm_version)
+#define DRM_IOCTL_GET_UNIQUE		DRM_IOWR(0x01, struct drm_unique)
+#define DRM_IOCTL_GET_MAGIC		DRM_IOR( 0x02, struct drm_auth)
+#define DRM_IOCTL_IRQ_BUSID		DRM_IOWR(0x03, struct drm_irq_busid)
+#define DRM_IOCTL_GET_MAP               DRM_IOWR(0x04, struct drm_map)
+#define DRM_IOCTL_GET_CLIENT            DRM_IOWR(0x05, struct drm_client)
+#define DRM_IOCTL_GET_STATS             DRM_IOR( 0x06, struct drm_stats)
+#define DRM_IOCTL_SET_VERSION		DRM_IOWR(0x07, struct drm_set_version)
+#define DRM_IOCTL_MODESET_CTL           DRM_IOW(0x08, struct drm_modeset_ctl)
+#define DRM_IOCTL_GEM_CLOSE		DRM_IOW (0x09, struct drm_gem_close)
+#define DRM_IOCTL_GEM_FLINK		DRM_IOWR(0x0a, struct drm_gem_flink)
+#define DRM_IOCTL_GEM_OPEN		DRM_IOWR(0x0b, struct drm_gem_open)
+#define DRM_IOCTL_GET_CAP		DRM_IOWR(0x0c, struct drm_get_cap)
+#define DRM_IOCTL_SET_CLIENT_CAP	DRM_IOW( 0x0d, struct drm_set_client_cap)
+#define DRM_IOCTL_SET_NAME	        DRM_IOW( 0x0e, struct drm_set_name)
+
+#define DRM_IOCTL_SET_UNIQUE		DRM_IOW( 0x10, struct drm_unique)
+#define DRM_IOCTL_AUTH_MAGIC		DRM_IOW( 0x11, struct drm_auth)
+#define DRM_IOCTL_BLOCK			DRM_IOWR(0x12, struct drm_block)
+#define DRM_IOCTL_UNBLOCK		DRM_IOWR(0x13, struct drm_block)
+#define DRM_IOCTL_CONTROL		DRM_IOW( 0x14, struct drm_control)
+#define DRM_IOCTL_ADD_MAP		DRM_IOWR(0x15, struct drm_map)
+#define DRM_IOCTL_ADD_BUFS		DRM_IOWR(0x16, struct drm_buf_desc)
+#define DRM_IOCTL_MARK_BUFS		DRM_IOW( 0x17, struct drm_buf_desc)
+#define DRM_IOCTL_INFO_BUFS		DRM_IOWR(0x18, struct drm_buf_info)
+#define DRM_IOCTL_MAP_BUFS		DRM_IOWR(0x19, struct drm_buf_map)
+#define DRM_IOCTL_FREE_BUFS		DRM_IOW( 0x1a, struct drm_buf_free)
+
+#define DRM_IOCTL_RM_MAP		DRM_IOW( 0x1b, struct drm_map)
+
+#define DRM_IOCTL_SET_SAREA_CTX		DRM_IOW( 0x1c, struct drm_ctx_priv_map)
+#define DRM_IOCTL_GET_SAREA_CTX 	DRM_IOWR(0x1d, struct drm_ctx_priv_map)
+
+#define DRM_IOCTL_SET_MASTER            DRM_IO(0x1e)
+#define DRM_IOCTL_DROP_MASTER           DRM_IO(0x1f)
+
+#define DRM_IOCTL_ADD_CTX		DRM_IOWR(0x20, struct drm_ctx)
+#define DRM_IOCTL_RM_CTX		DRM_IOWR(0x21, struct drm_ctx)
+#define DRM_IOCTL_MOD_CTX		DRM_IOW( 0x22, struct drm_ctx)
+#define DRM_IOCTL_GET_CTX		DRM_IOWR(0x23, struct drm_ctx)
+#define DRM_IOCTL_SWITCH_CTX		DRM_IOW( 0x24, struct drm_ctx)
+#define DRM_IOCTL_NEW_CTX		DRM_IOW( 0x25, struct drm_ctx)
+#define DRM_IOCTL_RES_CTX		DRM_IOWR(0x26, struct drm_ctx_res)
+#define DRM_IOCTL_ADD_DRAW		DRM_IOWR(0x27, struct drm_draw)
+#define DRM_IOCTL_RM_DRAW		DRM_IOWR(0x28, struct drm_draw)
+#define DRM_IOCTL_DMA			DRM_IOWR(0x29, struct drm_dma)
+#define DRM_IOCTL_LOCK			DRM_IOW( 0x2a, struct drm_lock)
+#define DRM_IOCTL_UNLOCK		DRM_IOW( 0x2b, struct drm_lock)
+#define DRM_IOCTL_FINISH		DRM_IOW( 0x2c, struct drm_lock)
+
+#define DRM_IOCTL_PRIME_HANDLE_TO_FD    DRM_IOWR(0x2d, struct drm_prime_handle)
+#define DRM_IOCTL_PRIME_FD_TO_HANDLE    DRM_IOWR(0x2e, struct drm_prime_handle)
+
+#define DRM_IOCTL_AGP_ACQUIRE		DRM_IO(  0x30)
+#define DRM_IOCTL_AGP_RELEASE		DRM_IO(  0x31)
+#define DRM_IOCTL_AGP_ENABLE		DRM_IOW( 0x32, struct drm_agp_mode)
+#define DRM_IOCTL_AGP_INFO		DRM_IOR( 0x33, struct drm_agp_info)
+#define DRM_IOCTL_AGP_ALLOC		DRM_IOWR(0x34, struct drm_agp_buffer)
+#define DRM_IOCTL_AGP_FREE		DRM_IOW( 0x35, struct drm_agp_buffer)
+#define DRM_IOCTL_AGP_BIND		DRM_IOW( 0x36, struct drm_agp_binding)
+#define DRM_IOCTL_AGP_UNBIND		DRM_IOW( 0x37, struct drm_agp_binding)
+
+#define DRM_IOCTL_SG_ALLOC		DRM_IOWR(0x38, struct drm_scatter_gather)
+#define DRM_IOCTL_SG_FREE		DRM_IOW( 0x39, struct drm_scatter_gather)
+
+#define DRM_IOCTL_WAIT_VBLANK		DRM_IOWR(0x3a, union drm_wait_vblank)
+
+#define DRM_IOCTL_CRTC_GET_SEQUENCE	DRM_IOWR(0x3b, struct drm_crtc_get_sequence)
+#define DRM_IOCTL_CRTC_QUEUE_SEQUENCE	DRM_IOWR(0x3c, struct drm_crtc_queue_sequence)
+
+#define DRM_IOCTL_UPDATE_DRAW		DRM_IOW(0x3f, struct drm_update_draw)
+
+#define DRM_IOCTL_MODE_GETRESOURCES	DRM_IOWR(0xA0, struct drm_mode_card_res)
+#define DRM_IOCTL_MODE_GETCRTC		DRM_IOWR(0xA1, struct drm_mode_crtc)
+#define DRM_IOCTL_MODE_SETCRTC		DRM_IOWR(0xA2, struct drm_mode_crtc)
+#define DRM_IOCTL_MODE_CURSOR		DRM_IOWR(0xA3, struct drm_mode_cursor)
+#define DRM_IOCTL_MODE_GETGAMMA		DRM_IOWR(0xA4, struct drm_mode_crtc_lut)
+#define DRM_IOCTL_MODE_SETGAMMA		DRM_IOWR(0xA5, struct drm_mode_crtc_lut)
+#define DRM_IOCTL_MODE_GETENCODER	DRM_IOWR(0xA6, struct drm_mode_get_encoder)
+#define DRM_IOCTL_MODE_GETCONNECTOR	DRM_IOWR(0xA7, struct drm_mode_get_connector)
+#define DRM_IOCTL_MODE_ATTACHMODE	DRM_IOWR(0xA8, struct drm_mode_mode_cmd) /* deprecated (never worked) */
+#define DRM_IOCTL_MODE_DETACHMODE	DRM_IOWR(0xA9, struct drm_mode_mode_cmd) /* deprecated (never worked) */
+
+#define DRM_IOCTL_MODE_GETPROPERTY	DRM_IOWR(0xAA, struct drm_mode_get_property)
+#define DRM_IOCTL_MODE_SETPROPERTY	DRM_IOWR(0xAB, struct drm_mode_connector_set_property)
+#define DRM_IOCTL_MODE_GETPROPBLOB	DRM_IOWR(0xAC, struct drm_mode_get_blob)
+#define DRM_IOCTL_MODE_GETFB		DRM_IOWR(0xAD, struct drm_mode_fb_cmd)
+#define DRM_IOCTL_MODE_ADDFB		DRM_IOWR(0xAE, struct drm_mode_fb_cmd)
+/**
+ * DRM_IOCTL_MODE_RMFB - Remove a framebuffer.
+ *
+ * This removes a framebuffer previously added via ADDFB/ADDFB2. The IOCTL
+ * argument is a framebuffer object ID.
+ *
+ * Warning: removing a framebuffer currently in-use on an enabled plane will
+ * disable that plane. The CRTC the plane is linked to may also be disabled
+ * (depending on driver capabilities).
+ */
+#define DRM_IOCTL_MODE_RMFB		DRM_IOWR(0xAF, unsigned int)
+#define DRM_IOCTL_MODE_PAGE_FLIP	DRM_IOWR(0xB0, struct drm_mode_crtc_page_flip)
+#define DRM_IOCTL_MODE_DIRTYFB		DRM_IOWR(0xB1, struct drm_mode_fb_dirty_cmd)
+
+#define DRM_IOCTL_MODE_CREATE_DUMB DRM_IOWR(0xB2, struct drm_mode_create_dumb)
+#define DRM_IOCTL_MODE_MAP_DUMB    DRM_IOWR(0xB3, struct drm_mode_map_dumb)
+#define DRM_IOCTL_MODE_DESTROY_DUMB    DRM_IOWR(0xB4, struct drm_mode_destroy_dumb)
+#define DRM_IOCTL_MODE_GETPLANERESOURCES DRM_IOWR(0xB5, struct drm_mode_get_plane_res)
+#define DRM_IOCTL_MODE_GETPLANE	DRM_IOWR(0xB6, struct drm_mode_get_plane)
+#define DRM_IOCTL_MODE_SETPLANE	DRM_IOWR(0xB7, struct drm_mode_set_plane)
+#define DRM_IOCTL_MODE_ADDFB2		DRM_IOWR(0xB8, struct drm_mode_fb_cmd2)
+#define DRM_IOCTL_MODE_OBJ_GETPROPERTIES	DRM_IOWR(0xB9, struct drm_mode_obj_get_properties)
+#define DRM_IOCTL_MODE_OBJ_SETPROPERTY	DRM_IOWR(0xBA, struct drm_mode_obj_set_property)
+#define DRM_IOCTL_MODE_CURSOR2		DRM_IOWR(0xBB, struct drm_mode_cursor2)
+#define DRM_IOCTL_MODE_ATOMIC		DRM_IOWR(0xBC, struct drm_mode_atomic)
+#define DRM_IOCTL_MODE_CREATEPROPBLOB	DRM_IOWR(0xBD, struct drm_mode_create_blob)
+#define DRM_IOCTL_MODE_DESTROYPROPBLOB	DRM_IOWR(0xBE, struct drm_mode_destroy_blob)
+
+#define DRM_IOCTL_SYNCOBJ_CREATE	DRM_IOWR(0xBF, struct drm_syncobj_create)
+#define DRM_IOCTL_SYNCOBJ_DESTROY	DRM_IOWR(0xC0, struct drm_syncobj_destroy)
+#define DRM_IOCTL_SYNCOBJ_HANDLE_TO_FD	DRM_IOWR(0xC1, struct drm_syncobj_handle)
+#define DRM_IOCTL_SYNCOBJ_FD_TO_HANDLE	DRM_IOWR(0xC2, struct drm_syncobj_handle)
+#define DRM_IOCTL_SYNCOBJ_WAIT		DRM_IOWR(0xC3, struct drm_syncobj_wait)
+#define DRM_IOCTL_SYNCOBJ_RESET		DRM_IOWR(0xC4, struct drm_syncobj_array)
+#define DRM_IOCTL_SYNCOBJ_SIGNAL	DRM_IOWR(0xC5, struct drm_syncobj_array)
+
+#define DRM_IOCTL_MODE_CREATE_LEASE	DRM_IOWR(0xC6, struct drm_mode_create_lease)
+#define DRM_IOCTL_MODE_LIST_LESSEES	DRM_IOWR(0xC7, struct drm_mode_list_lessees)
+#define DRM_IOCTL_MODE_GET_LEASE	DRM_IOWR(0xC8, struct drm_mode_get_lease)
+#define DRM_IOCTL_MODE_REVOKE_LEASE	DRM_IOWR(0xC9, struct drm_mode_revoke_lease)
+
+#define DRM_IOCTL_SYNCOBJ_TIMELINE_WAIT	DRM_IOWR(0xCA, struct drm_syncobj_timeline_wait)
+#define DRM_IOCTL_SYNCOBJ_QUERY		DRM_IOWR(0xCB, struct drm_syncobj_timeline_array)
+#define DRM_IOCTL_SYNCOBJ_TRANSFER	DRM_IOWR(0xCC, struct drm_syncobj_transfer)
+#define DRM_IOCTL_SYNCOBJ_TIMELINE_SIGNAL	DRM_IOWR(0xCD, struct drm_syncobj_timeline_array)
+
+/**
+ * DRM_IOCTL_MODE_GETFB2 - Get framebuffer metadata.
+ *
+ * This queries metadata about a framebuffer. User-space fills
+ * &drm_mode_fb_cmd2.fb_id as the input, and the kernels fills the rest of the
+ * struct as the output.
+ *
+ * If the client is DRM master or has &CAP_SYS_ADMIN, &drm_mode_fb_cmd2.handles
+ * will be filled with GEM buffer handles. Planes are valid until one has a
+ * zero handle -- this can be used to compute the number of planes.
+ *
+ * Otherwise, &drm_mode_fb_cmd2.handles will be zeroed and planes are valid
+ * until one has a zero &drm_mode_fb_cmd2.pitches.
+ *
+ * If the framebuffer has a format modifier, &DRM_MODE_FB_MODIFIERS will be set
+ * in &drm_mode_fb_cmd2.flags and &drm_mode_fb_cmd2.modifier will contain the
+ * modifier. Otherwise, user-space must ignore &drm_mode_fb_cmd2.modifier.
+ */
+#define DRM_IOCTL_MODE_GETFB2		DRM_IOWR(0xCE, struct drm_mode_fb_cmd2)
+
+/*
+ * Device specific ioctls should only be in their respective headers
+ * The device specific ioctl range is from 0x40 to 0x9f.
+ * Generic IOCTLS restart at 0xA0.
+ *
+ * \sa drmCommandNone(), drmCommandRead(), drmCommandWrite(), and
+ * drmCommandReadWrite().
+ */
+#define DRM_COMMAND_BASE                0x40
+#define DRM_COMMAND_END			0xA0
+
+/*
+ * Header for events written back to userspace on the drm fd.  The
+ * type defines the type of event, the length specifies the total
+ * length of the event (including the header), and user_data is
+ * typically a 64 bit value passed with the ioctl that triggered the
+ * event.  A read on the drm fd will always only return complete
+ * events, that is, if for example the read buffer is 100 bytes, and
+ * there are two 64 byte events pending, only one will be returned.
+ *
+ * Event types 0 - 0x7fffffff are generic drm events, 0x80000000 and
+ * up are chipset specific.
+ */
+struct drm_event {
+	__u32 type;
+	__u32 length;
+};
+
+#define DRM_EVENT_VBLANK 0x01
+#define DRM_EVENT_FLIP_COMPLETE 0x02
+#define DRM_EVENT_CRTC_SEQUENCE	0x03
+
+struct drm_event_vblank {
+	struct drm_event base;
+	__u64 user_data;
+	__u32 tv_sec;
+	__u32 tv_usec;
+	__u32 sequence;
+	__u32 crtc_id; /* 0 on older kernels that do not support this */
+};
+
+/* Event delivered at sequence. Time stamp marks when the first pixel
+ * of the refresh cycle leaves the display engine for the display
+ */
+struct drm_event_crtc_sequence {
+	struct drm_event	base;
+	__u64			user_data;
+	__s64			time_ns;
+	__u64			sequence;
+};
+
+/* typedef area */
+typedef struct drm_clip_rect drm_clip_rect_t;
+typedef struct drm_drawable_info drm_drawable_info_t;
+typedef struct drm_tex_region drm_tex_region_t;
+typedef struct drm_hw_lock drm_hw_lock_t;
+typedef struct drm_version drm_version_t;
+typedef struct drm_unique drm_unique_t;
+typedef struct drm_list drm_list_t;
+typedef struct drm_block drm_block_t;
+typedef struct drm_control drm_control_t;
+typedef enum drm_map_type drm_map_type_t;
+typedef enum drm_map_flags drm_map_flags_t;
+typedef struct drm_ctx_priv_map drm_ctx_priv_map_t;
+typedef struct drm_map drm_map_t;
+typedef struct drm_client drm_client_t;
+typedef enum drm_stat_type drm_stat_type_t;
+typedef struct drm_stats drm_stats_t;
+typedef enum drm_lock_flags drm_lock_flags_t;
+typedef struct drm_lock drm_lock_t;
+typedef enum drm_dma_flags drm_dma_flags_t;
+typedef struct drm_buf_desc drm_buf_desc_t;
+typedef struct drm_buf_info drm_buf_info_t;
+typedef struct drm_buf_free drm_buf_free_t;
+typedef struct drm_buf_pub drm_buf_pub_t;
+typedef struct drm_buf_map drm_buf_map_t;
+typedef struct drm_dma drm_dma_t;
+typedef union drm_wait_vblank drm_wait_vblank_t;
+typedef struct drm_agp_mode drm_agp_mode_t;
+typedef enum drm_ctx_flags drm_ctx_flags_t;
+typedef struct drm_ctx drm_ctx_t;
+typedef struct drm_ctx_res drm_ctx_res_t;
+typedef struct drm_draw drm_draw_t;
+typedef struct drm_update_draw drm_update_draw_t;
+typedef struct drm_auth drm_auth_t;
+typedef struct drm_irq_busid drm_irq_busid_t;
+typedef enum drm_vblank_seq_type drm_vblank_seq_type_t;
+
+typedef struct drm_agp_buffer drm_agp_buffer_t;
+typedef struct drm_agp_binding drm_agp_binding_t;
+typedef struct drm_agp_info drm_agp_info_t;
+typedef struct drm_scatter_gather drm_scatter_gather_t;
+typedef struct drm_set_version drm_set_version_t;
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif
diff --git a/src/drm/drm_renderer.c b/src/drm/drm_renderer.c
index c6c0c5ba..73792828 100644
--- a/src/drm/drm_renderer.c
+++ b/src/drm/drm_renderer.c
@@ -21,6 +21,10 @@
 #  include "msm/msm_renderer.h"
 #endif
 
+#ifdef ENABLE_DRM_AMDGPU
+#  include "amdgpu/amdgpu_renderer.h"
+#endif
+
 static struct virgl_renderer_capset_drm capset;
 
 static const struct backend {
@@ -37,6 +41,14 @@ static const struct backend {
       .create = msm_renderer_create,
    },
 #endif
+#ifdef ENABLE_DRM_AMDGPU
+   {
+      .context_type = VIRTGPU_DRM_CONTEXT_AMDGPU,
+      .name = "amdgpu",
+      .probe = amdgpu_renderer_probe,
+      .create = amdgpu_renderer_create,
+   },
+#endif
 };
 
 int
diff --git a/src/drm/drm_renderer.h b/src/drm/drm_renderer.h
index be7090de..9ff4449e 100644
--- a/src/drm/drm_renderer.h
+++ b/src/drm/drm_renderer.h
@@ -14,7 +14,7 @@
 
 #include "virgl_util.h"
 
-#ifdef ENABLE_DRM_MSM
+#if defined(ENABLE_DRM_MSM) || defined(ENABLE_DRM_AMDGPU)
 
 int drm_renderer_init(int drm_fd);
 
diff --git a/src/drm_hw.h b/src/drm_hw.h
index 45a2f4f4..96c3fa12 100644
--- a/src/drm_hw.h
+++ b/src/drm_hw.h
@@ -6,13 +6,18 @@
 #ifndef DRM_HW_H_
 #define DRM_HW_H_
 
+#ifdef ENABLE_DRM_AMDGPU
+#include <amdgpu.h>
+#endif
+
 struct virgl_renderer_capset_drm {
    uint32_t wire_format_version;
    /* Underlying drm device version: */
    uint32_t version_major;
    uint32_t version_minor;
    uint32_t version_patchlevel;
-#define VIRTGPU_DRM_CONTEXT_MSM   1
+#define VIRTGPU_DRM_CONTEXT_MSM      1
+#define VIRTGPU_DRM_CONTEXT_AMDGPU   2
    uint32_t context_type;
    uint32_t pad;
    union {
@@ -27,6 +32,15 @@ struct virgl_renderer_capset_drm {
          uint64_t chip_id;
          uint32_t max_freq;
       } msm;  /* context_type == VIRTGPU_DRM_CONTEXT_MSM */
+      struct {
+         uint32_t address32_hi;
+         uint32_t __pad;
+#ifdef ENABLE_DRM_AMDGPU
+         struct amdgpu_buffer_size_alignments alignments;
+         struct amdgpu_gpu_info gpu_info;
+#endif
+         char marketing_name[128];
+      } amdgpu;   /* context_type == VIRTGPU_DRM_CONTEXT_AMDGPU */
    } u;
 };
 
diff --git a/src/meson.build b/src/meson.build
index d5bf424c..4835fb2d 100644
--- a/src/meson.build
+++ b/src/meson.build
@@ -156,6 +156,12 @@ drm_msm_sources = [
    'drm/msm/msm_renderer.h',
 ]
 
+drm_amdgpu_sources = [
+   'drm/drm-uapi/amdgpu_drm.h',
+   'drm/amdgpu/amdgpu_renderer.c',
+   'drm/amdgpu/amdgpu_renderer.h',
+]
+
 proxy_sources = [
    'proxy/proxy_client.c',
    'proxy/proxy_common.c',
@@ -223,6 +229,11 @@ if with_drm_msm
    virgl_sources += drm_msm_sources
 endif
 
+if with_drm_amdgpu
+   virgl_sources += drm_amdgpu_sources
+   virgl_depends += [libdrm_amdgpu_dep]
+endif
+
 if with_render_server
    virgl_sources += proxy_sources
 endif
diff --git a/src/virglrenderer.c b/src/virglrenderer.c
index 5ae375f7..5af457f4 100644
--- a/src/virglrenderer.c
+++ b/src/virglrenderer.c
@@ -87,7 +87,7 @@ static int virgl_renderer_resource_create_internal(struct virgl_renderer_resourc
    struct vrend_renderer_resource_create_args vrend_args =  { 0 };
    uint32_t map_info;
 
-   if (!state.vrend_initialized)
+   if (!state.vrend_initialized && !state.drm_initialized)
       return EINVAL;
 
    /* do not accept handle 0 */
diff --git a/src/virglrenderer.h b/src/virglrenderer.h
index 815c952d..e871a4f9 100644
--- a/src/virglrenderer.h
+++ b/src/virglrenderer.h
@@ -48,7 +48,7 @@ struct virgl_renderer_gl_ctx_param {
    int compat_ctx;
 };
 
-#define VIRGL_RENDERER_CALLBACKS_VERSION 4
+#define VIRGL_RENDERER_CALLBACKS_VERSION 5
 
 struct virgl_renderer_callbacks {
    int version;
diff --git a/src/vrend_decode.c b/src/vrend_decode.c
index 69e4d1af..632e5295 100644
--- a/src/vrend_decode.c
+++ b/src/vrend_decode.c
@@ -1636,7 +1636,7 @@ struct virgl_context *vrend_renderer_context_create(uint32_t handle,
 {
    struct vrend_decode_ctx *dctx;
 
-   dctx = malloc(sizeof(struct vrend_decode_ctx));
+   dctx = calloc(1, sizeof(struct vrend_decode_ctx));
    if (!dctx)
       return NULL;
 
-- 
2.17.1

